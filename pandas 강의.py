# 데이터 변환하기 1

## 데이터 불러오기

실습에 사용할 데이터를 불러오겠습니다.

import pandas as pd

#파일불러오기
df=pd.read_csv('./data/seoul_park.csv')
df.head()

데이터를 다시 살펴보겠습니다.

df

데이터의 숫자들에는 천 단위로 콤마(,)가 찍혀있습니다. 또한 데이터의 "단체" 컬럼의 윗부분과 아래부분을 보면 윗쪽의 데이터에는 0명이 **0** 으로 적혀있지만, 아래쪽에는 **-** 으로 표기되어있습니다. 이는 년도가 지나면서 기록 방식이 바뀌면서 생긴 문제입니다.

df.info()

`info()`를 사용해 데이터의 타입을 확인해보면 모든 데이터가 수가 아닌 텍스트(Object) 타입으로 저장되어 있습니다. 이는 즉 데이터에 있는 3,359는 숫자 3359가 아닌 텍스트 "3,359"가 저장되어있는 상태입니다. 이 상태로는 숫자의 연산, 나아가 평균값과 같은 통계량을 측정할 수 없습니다. 따라서 우리는 이 텍스트(Object)들을 모두 **정수형(int)** 으로 바꾸어야 합니다.

정수형으로 바꾸기 위해선 텍스트에서 숫자만을 남기고 모두 제거해야 합니다. 이를 위해 숫자로 바꾸어야 하는 컬럼들에서 `str.replace()`를 활용하여 -를 0으로 바꾸고 콤마를 제거합니다.
유료합계~ 총계까지 모두 숫자로 바꾸어야함


# 쉽게 데이터 바꾸기
columns=['유료합계', '어른', '청소년', '어린이', '외국인', '단체', '무료합계', '총계']
for i in columns:
    df[i]=df[i].str.replace('-','0')  #- 가 보이면 0으로 바꾼다
    df[i]=df[i].str.replace(',','')  # , 가 보이면 빈칸으로 

df.head()

데이터의 숫자들이 깔끔하게 정리된 것을 확인할 수 있습니다.

## 데이터 타입 변환: astype()

df.info()

데이터의 숫자들이 깔끔하게 정리되기는 했지만, `info`를 활용하여 살펴보면 여전히 데이터 타입은 텍스트(Object)인 것을 알 수 있습니다. 통계적인 분석을 위해 해당 데이터들을 정수(int)형태로 바꿔주어야 합니다.
object = str 비슷

`astype()`을 사용하면 원하는 타입으로 변환할 수 있습니다. `astype`메서드를 활용해 "어른" 컬럼의 데이터를 정수 형태로 변환하는 코드는 다음과 같습니다.

df["어른"].astype(int)  
# 내부함수만 사용했지, 대입연산자로 기존 컬럼을 덮어주지는 않았다
# 다음 info에서 여전히 object로 되어 있다

이제 "어른" 컬럼의 형태가 제대로 변환되었는지 확인해보겠습니다.

df.info()

`df`의 어른 컬럼의 데이터타입은 object로, `astype()`을 사용했지만 바뀌지 않은 것을 확인할 수 있습니다. 어떻게 된 일일까요.

`astype()`을 비롯해 데이터프레임에 뭔가 변형을 가하거나 작업을 하는 메서드들은 데이터프레임 자체를 변환하지 않고, 변환된 새로운 데이터프레임을 반환합니다. 

따라서 `df["어른"].astype(int)`이라는 코드는 `df`의 "어른" 컬럼을 정수형으로 변환하기는 하지만, 그냥 정수형으로 바뀐 "어른" 컬럼을 시리즈 형태로 나타낼 뿐 **`df`의 "어른" 컬럼 그 자체가 바뀌는 것이 아닙니다**. 

`df`의 "어른" 컬럼을 바꾸고 싶다면 `astype()`을 활용해 변환하여 생성한 "어른" 컬럼 시리즈를 `df`의 "어른" 컬럼에 덮어씌워주는 작업이 필요합니다. 이는 다음과 같이 수행할 수 있습니다.

df["어른"]=df["어른"].astype(int) 
# 대입연산자로 원본을 덮어씌우기 해줘야 Dtype 이 바뀐다

df.info()

덮어씌워주는 과정을 통해 어른 컬럼의 데이터타입이 정수형(int)으로 바뀐 것을 확인할 수 있습니다. 이런 식으로 원본의 데이터프레임을 가공하기 위해선 덮어씌워주는 작업이 필요하다는 사실과 해당 코드를 익혀두시면 앞으로 강의의 코드를 이해하시기 한결 수월할 것입니다. int 64 64비트

## 데이터 타입 변환: to_numeric()

`to_numeric()`은 `astype()`과 같이 데이터의 형태를 변환하지만 원하는 타입을 지정할 수 있는 `astype()`과 달리 숫자로만 변환한다는 차이점이 존재합니다. 특히나 데이터분석에서는 데이터를 숫자로 변환할 일이 많기 때문에 유용하게 사용할 수 있습니다.

df["유료합계"] = pd.to_numeric(df["유료합계"])

df.info()

유료합계 컬럼이 정수형(int)으로 바뀐 것을 확인할 수 있습니다.


### [TODO] `df`의 "외국인" 컬럼의 자료형을 숫자형으로 변환해 변수 `foreigner`에 저장하세요.
* `astype()` 또는 `to_numeric()`을 활용하세요.

foreigner=df["외국인"].astype(int)
foreigner=pd.to_numeric(df["외국인"])
print(foreigner.info())
# print(foreigner.info()) # 주석을 해제하고 실행해 올바르게 형변환이 되어 저장됐는지 확인하세요.

이제 for문을 사용해 입장객 수에 관련된 컬럼들을 모두 숫자형으로 바꿔보도록 하겠습니다.

columns=["유료합계", "어른", "청소년", "어린이", "단체", "무료합계", "총계"]
for i in columns:
    df[i]=pd.to_numeric(df[i])
    
df.info()

입장객 수에 관련된 데이터가 모두 숫자형태로 바뀐 것을 확인할 수 있습니다. 이제 이 입장객 수를 활용해 평균, 최대값 등을 계산할 수 있습니다.

## 데이터 타입 변환: to_datetime()

날짜 컬럼에는 데이터가 수집된 날짜가 기록되어있습니다. 하지만 위의 정보를 보면 알 수 있다시피 날짜는 텍스트로 저장되어있습니다. 다시 말해 날짜 컬럼의 "2016-01-01"는 텍스트일 뿐 컴퓨터가 이것을 날짜로 인식할 수 없습니다. 이 날짜를 보다 효과적으로 활용하기 위해 `to_datetime()`을 활용해 날짜컬럼 전체를 `datetime`형식으로 변환해줍니다.

df["날짜"]= pd.to_datetime(df["날짜"]) 
df.info()

이번엔 날짜 데이터를 사용해보겠습니다. 데이터프레임에서 `datetime` 형태의 데이터에는 연, 월, 일 정보가 담겨있어 이를 활용하면 날짜정보를 비롯해 요일과 같은 새로운 정보를 생성할 수 있습니다. 

이를 위해 `dt` 속성을 사용하여 `datetime` 형태의 데이터에서 특정 정보들을 추출하고 연, 월, 일, 요일 컬럼을 새롭게 생성해보겠습니다.

#위에서 datetime으로 바꾸었다
df['연']=df['날짜'].dt.year   
df['월']=df['날짜'].dt.month
df['일']=df['날짜'].dt.day
df['요일']=df['날짜'].dt.dayofweek
# dt 는 datetime 데이터 타입에만 사용할수 있다
# 새로운 연,월,일,요일 없던 컬럼이 생성된다

df.head()

연, 월, 일, 요일 컬럼이 생성되고 해당하는 정보가 저장된 것을 확인할 수 있습니다. 그런데 요일이 글자가 아닌 **숫자**로 기록되어 있는 것을 확인할 수 있습니다. 

이대로도 데이터 분석을 진행할 수는 있지만, 조금 알아보기 번거롭습니다. 만약 요일에 있는 숫자들을 한글로 바꾸는 것 처럼 데이터에 일괄적으로 변형을 주고 싶다면 어떻게 해야할까요? 이에 대해서는 다음 실습에서 알아보도록 하겠습니다.

## 채점
* **[TODO]** 중 수행하지 않은 부분이 없는지 확인하세요.
* 채점을 위해 아래 코드를 실행한 뒤 우측 상단의 제출 버튼을 눌러주세요.
* 코드 수정시 정상적인 채점이 이루어지지 않습니다.

import os
import json

foreigner.to_json("result.json")
##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################




##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################




##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################

# 데이터 변환하기 2

## 데이터 불러오기

이전 실습에서 우리는 데이터의 형 변환을 통해 숫자와 날짜 데이터들의 형태를 변환해주고, 날짜 데이터를 활용해 연, 월, 일, 요일 컬럼을 생성하였습니다. 지난 실습을 통해 정제된 데이터를 불러옵니다.

import pandas as pd

df=pd.read_csv("./data/seoul_park02.csv")
df.head()

요일 컬럼이 글자가 아닌 숫자로 기록되어 있는 것을 확인할 수 있습니다. 이대로도 데이터 분석을 진행할 수는 있지만, 알아보기가 조금 어렵습니다. 

## 열 전체 변환: map()

이 숫자들은 0부터 6까지, 월요일부터 일요일까지 숫자가 매칭되어있습니다. 이 정보를 활용하여 "요일" 컬럼 전체 데이터를 변환하고자 `map()`을 사용합니다.

먼저 `map()`에 변환 정보를 입력해주기 위해 숫자를 키로, 그에 대응하는 요일 글자를 값으로 가지는 `week` 딕셔너리를 생성합니다.

week={0:'월', 1:'화', 2:'수', 3:'목', 4:'금', 5:'토',6:'일'}

이제 `map()`과 요일 정보가 담긴 `week` 딕셔너리를 사용해서 `df`의 "요일" 컬럼을 변환합니다.

df['요일']=df['요일'].map(week)  # 대입연산자로 덮어쓰기 해줘야한다
df.head()

요일 컬럼의 데이터들이 딕셔너리에 맞춰 변환된 것을 확인할 수 있습니다.

## 데이터에 함수 적용: apply()

이번엔 "날씨" 컬럼을 살펴보겠습니다.

df['날씨'].value_counts()

날씨 컬럼 데이터에는  '눈', '비', '눈/비' 가 있습니다. 날씨와 입장객수의 관계는 매우 밀접하지만, 이것은 어디까지나 날씨가 맑은지 흐린지가 중요할 뿐 비인지 눈인지는 크게 중요하지 않습니다. 그렇기 때문에 세 종류의 데이터를 모두 '눈/비'로 통일해서 합치고자 합니다.

먼저 '눈', '비' 를 입력받으면 '눈/비'로 바꿔주는 함수를 선언합니다.

# 함수를 쓰는 방법
# 1 .함수 : 특정한 기능을 하는 코드의 모임
# 2. 메서드 : 특정한 데이터에만 적용할수 있는 함수
# 함수가 좀더 메서드보다 상위 포함 관계
#  print()    함수는 print로 표현?
#  list.append()  : 메서드 예시   list의 메서드는 insert, append, sort 등 ,,,


# 눈 또는 비라는 입력이 있다면, 눈/비 출력
# 눈 비 모두 아니라면 자기자신을 출력
#매개변수=인자
# def 함수명(매개변수):
def weather(e):
    if e=='눈' or e=='비':
        return '눈/비'
    else:
        return e

이제 이 함수를 컬럼 전체에 적용하기 위해 `apply()`를 사용합니다. `apply()`는 특정 컬럼 혹은 데이터프레임 전체에 특정 함수를 적용할 때 사용 가능합니다. 

`df`의 "날씨" 컬럼에 `weather` 함수를 적용하려면 아래와 같이 사용할 수 있습니다.

df["날씨"]=df["날씨"].apply(weather)

df["날씨"].value_counts()

"날짜" 컬럼의 데이터에서 '눈', '비'가 '눈/비'로 통일되어 111개가 되었음을 확인할 수 있습니다.

이렇게 원하는 함수를 정의해서 `apply()`를 활용해 적용하는 과정은 굉장히 많이 사용되게 됩니다. 하지만 다양한 변환이 필요할수록 매번 함수를 만들고 사용해야하고, 이는 코드를 지저분하게 만듭니다.

파이썬에서는 이러한 문제를 해결하기 위해 일회용으로 사용할 함수를 정의하는데 유용한 **람다함수** 기능을 지원합니다. 람다함수를 사용하면 위에서 `weather()` 함수를 정의하는 과정을 생략하고 다음과 같이 사용할 수 있습니다.

# 람다함수 쓰는 방법
df["날씨"]=df["날씨"].apply(lambda e : "눈/비" if e=="눈" or e=="비" else e)

### [TODO] 일차별 혼잡도 시리즈를 생성한 뒤 변수 `congestion`에 저장하세요.
* `apply()`를 사용하세요.
* "총계" 컬럼의 값이 8000 이상인 날은 "혼잡", 8000 미만인 날은 "원활" 로 합니다.
* 평범하게 함수를 정의해도 되고, 람다함수를 활용해도 됩니다.



# None을 지우고 알맞은 코드를 입력하세요.
# 총계 컬럼 값이 8000이상이면 혼잡을 반환
# 8000 미만이면, 원할 반환
# 바뀌는 값은 return으로 반환

# def 함수명(x):
#    x의 값에 따라서
#    return(반환) 값은 바꿔주는 함수
# congestion=df['총계'].apply(함수명)



def sum(e):
    if e > 8000:
        return "혼잡"
    else:
        return "원활"
    
congestion=df['총계'].apply(sum)
congestion.value_counts()

## 채점
* **[TODO]** 중 수행하지 않은 부분이 없는지 확인하세요.
* 채점을 위해 아래 코드를 실행한 뒤 우측 상단의 제출 버튼을 눌러주세요.
* 코드 수정시 정상적인 채점이 이루어지지 않습니다.

import os
import json

congestion.to_json("result.json")


##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################




##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################




##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################


# 데이터 요약하기

## 데이터 불러오기

지난 실습에서 데이터 변환을 통해 "요일", "날씨" 컬럼을 알아보기 쉽게 변환하였습니다. 지난 실습을 통해 정제된 데이터를 불러옵니다.

import pandas as pd

df=pd.read_csv("./data/seoul_park03.csv")
mm=pd.read_csv("./data/misemunji.csv")

df.head()

## 데이터 통계값 확인: mean(), min(), max(), median()

데이터의 통계값을 확인하기 위해 각종 집계함수(`mean()`, `min()`, `max()`, `median()` 등)를 사용할 수 있습니다. 예를들어 `mean()`을 활용해 "어른" 컬럼의 평균값을 구하려면 다음과 같이 사용할 수 있습니다.

df["어른"].mean()

전체 일차의 어른 입장객 수의 평균을 구할 수 있습니다. 이번엔 총 입장객 수("총계")가 가장 적은 날의 입장객 수를 알아보도록 하겠습니다.

df["총계"].min()

총 인원이 2명만 입장한 날이 있다는 사실을 알 수 있습니다. 여담이지만 해당 날짜인 2017년 3월 28일은 조류독감으로 인해 폐쇄되었던 서울대공원의 재개장을 알리는 언론보도가 나온 날짜로 재개장을 위한 입장 및 집계 시스템의 내부 테스트 정도로 유추해볼 수 있습니다.

### [TODO] 집계함수를 사용해 미세먼지가 가장 높은 날의 수치를 변수 `badair`에 저장합니다.
* "미세먼지" 컬럼 값이 가장 높은 날의 수치를 저장합니다.
* 최대값을 구하는 집계함수는 `max()` 입니다.

# 변수 mm에는 미세먼지 데이터가 저장되어 있습니다.
mm.head()


# None을 지우고 알맞은 코드를 입력하세요.
badair=mm["미세먼지"].max()
badair

## 데이터 전체 통계: describe()

위에서 배운 집계함수를 활용하면 내가 원하는 통계값을 확인할 수 있지만, 때로는 전체 통계값을 보고 이를 통해 데이터의 분석 방향을 결정하기도 합니다. 이를 위해 데이터프레임의 다양한 통계값을 정리해서 보여주는 `describe()`를 사용합니다.

`describe()`는 숫자형 데이터들로 이루어진 컬럼들의 데이터의 갯수(count), 평균(mean), 표준편차(std), 최소값(min), 사분위수(25/50/75%), 최대값(max)를 보여줍니다.

df.describe()

`describe()`를 사용해 우리는 입장객의 수가 가장 많았던 날은 58,688명이었다는 것("총계" 컬럼의 max값), 평균적으로 어른, 청소년, 어린이, 외국인 순으로 입장객의 수가 많았다는 것(각 컬럼의 mean값 비교) 등 다양한 정보를 얻을 수 있습니다.

## 데이터 그룹화: groupby()

다음은 데이터를 그룹으로 묶어서 확인할 수 있는 `groupby()`입니다. `groupby()`를 사용하면 특정 기준에 따라 데이터를 정리해서 분석할 수 있습니다. 
예를 들어 서울대공원 데이터를 통해서 날씨에 따른 입장객의 수가 궁금할 때, 우리는 `groupby()`를 사용해서 "날씨"를 기준으로 "총계"값의 평균을 구할 수 있습니다. 

df.groupby("날씨")["총계"].mean()   #날씨를 기준으로 총계값의 평균을 구한다

예상대로 **날씨가 맑음일 때 가장 평균 입장객 수가 많고, 눈/비일 때 가장 적은 것**을 확인할 수 있습니다. 더 나아가 **날씨가 맑을때와 눈/비가 올 때의 입장객 수가 수치적으로 2배가 넘게 차이가 난다는 것**은 물론 **구름의 많고적음은 입장객의 수에 큰 영향을 끼치지 않는다는 사실**까지 알 수 있습니다. 

이렇게 `groupby()`를 사용하면 데이터를 그룹별로 분할하고 각 그룹에 대한 통계량을 확인하여 특정 기준에 따른 데이터의 추세를 확인할 수 있습니다.

이 데이터를 분할하는 기준은 한 개 이상이 될 수도 있습니다. 앞서 날씨에 따라 입장객의 총 수를 확인하였는데, 입장객의 수에 큰 영향을 미치는 또 하나의 요인은 바로 공휴일 여부입니다. `groupby()`에서 그룹화 기준이 되는 컬럼을 "날씨"와 "공휴일" 2개의 컬럼으로 설정하면 전체 데이터를 날씨에 따라서 그룹으로 한번 묶고, 각 날씨별 그룹 안에서 다시 공휴일 여부에 따라 두 분류로 데이터를 분류합니다.

df.groupby(["날씨","공휴일"])["총계"].mean()  

df.groupby(["날씨","공휴일"])[["총계"]].mean()  
# ["총계"]에 대괄호 [ ] 적어주면 깔끔하게 나온다

날씨에 따라 묶었던 데이터를 공휴일 여부에 따라서 데이터를 한번 더 분류했더니 좀 더 자세한 정보들을 알 수 있습니다. **공휴일이 아닌 날의 경우 날씨에 따라 입장객의 수 차이가 그리 크지 않다는 사실**을 알 수 있습니다. 또한 **날씨가 맑음이거나 구름일 때에는 공휴일 여부에 입장객의 수가 따라 월등히 차이가 나지만 눈/비 일때는 오히려 날씨에 따른 입장객 수의 차이가 크지 않음**을 알 수 있습니다.

`groupby()`를 활용해 2개 이상의 컬럼을 확인하는 것 역시 가능합니다. 이 경우 집계 결과가 데이터프레임으로 출력됩니다.


df.groupby(["날씨","공휴일"])[["어른", "어린이"]].mean()

### [TODO] 목요일이면서 공휴일인 날짜의 어른 입장객 수 최대값을 변수 `thursday`에 저장하세요.
* `groupby()`를 사용해 "요일" 컬럼과 "공휴일" 컬럼으로 데이터를 묶을 수 있습니다.
* 최대값을 구하기 위한 집계함수는 `max()` 입니다.

# groupby를 활용하여 "요일", "공휴일" 컬럼으로 데이터를 묶고 "어른"입장객 수 최대값을 확인합니다.
df.groupby(["요일","공휴일"])[["어른"]].max()  

# None을 지우고 공휴일인 목요일의 최대 어른 입장객 수를 저장하세요. 
thursday=38511

# 특정 컬럼에 해당하는 조건 가져오기
# thursday = df[특정 컬럼에 해당하는 조건]
# 1. 공휴일
a=df['공휴일']=='O'
b=df['요일']=='목'
thursday=df[a&b]['어른'].max()   # and 조건 '&', or 조건 shft + \ = '|'
thursday

## 채점
* **[TODO]** 중 수행하지 않은 부분이 없는지 확인하세요.
* 채점을 위해 아래 코드를 실행한 뒤 우측 상단의 제출 버튼을 눌러주세요.
* 코드 수정시 정상적인 채점이 이루어지지 않습니다.

import os
import json

result = {}
result["problem_1"] = int(badair)
result["problem_2"] = thursday

with open("result.json", "w") as f:
    json.dump(result, f)


##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################




##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################




##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################




####  1강 실습문제  ####################################################################################################################
1. pandas 패키지 불러오기
파이썬에서 import 문을 사용하면 사전에 설치된 라이브러리를 불러올 수 있습니다.

라이브러리는 일종의 확장팩 같은 개념으로, 파이썬에는 수많은 라이브러리가 존재합니다. 사용자는 import 문으로 불러오는 것만으로 그 기능들을 사용할 수 있습니다.


지시사항
pandas 라이브러리를 pd 라는 이름으로 불러옵니다.

불러온 pandas 라이브러리를 활용하여 비어있는 데이터프레임(DataFrame) 을 변수 df에 저장합니다.

# 지시사항을 수행해보세요.
import pandas as pd

df=pd.DataFrame()
#ser = pd.Series()

#df = Non # 지시사항을 수행해보세요.
print(df)
#print(ser)

2. Series 데이터
Series 데이터란 NumPy array가 보강된 형태의 Data와 index를 가지고 있는 pandas의 데이터 형식입니다.

Series 데이터를 만들기 위해서는 값과 인덱스를 부여해주어야 합니다.

series = pd.Series([1, 2, 3, 4], index=["a", "b", "c", "d"], name="Title")
Copy
이 시리즈 데이터는 1, 2, 3, 4 라는 값을 가지며 각각 "a", "b", "c", "d" 인덱스로 구분합니다. 그리고 이 시리즈 데이터의 이름은 Title입니다.


지시사항
시리즈 데이터를 만드는 법을 참고하여 my_series 에 시리즈 데이터를 만들고 출력하세요.

my_series의 데이터는 다음과 같습니다.
[100, 96, 80, 85]
Copy
my_series의 인덱스는 다음과 같습니다.
["Kor", "Eng", "Mat", "Sci"]
Copy
my_series의 이름은 다음과 같습니다.
Exam
Copy
새롭게 생성한 시리즈 출력 예시

Kor    100
Eng     96
Mat     80
Sci     85
Name: Exam, dtype: int64

import pandas as pd

# 인덱스가 자동으로 등록
# list_a =[1,2,3,4]
# 0번 인덱스 : 1
# 1번 인덱스 : 2
# 2번 인덱스 : 3
# 3번 인덱스 : 4
# print(list_a[0])



# 예시: 시리즈 데이터를 만드는 방법.
series = pd.Series([1, 2, 3, 4], index=["a", "b", "c", "d"], name="Title")
print(series, "\n")

# 지시사항을 참고하여 코드를 작성하세요.
my_series = pd.Series([100,96,80,85], index=["kor","Eng", "Mat","Sci"], name="Exam")
print(my_series, "\n")


3. 데이터프레임에서 특정 컬럼 조회
데이터프레임의 컬럼 이름을 사용하면 특정 칼럼을 조회할 수 있습니다.

컬럼 하나를 선택할 경우, pandas의 Series형태로 반환됩니다.


지시사항
학생의 이름, 국어, 수학, 영어 성적을 입력으로 받고, 이를 저장한 데이터프레임 df 에서 국어 컬럼을 조회해 출력해 보세요.

입력 예시

실행 후 Copy 버튼을 누르시면 아래 예시가 자동으로 입력됩니다.

3
홍길동 80 70 80
박철수 60 70 80
김영희 70 90 80
Copy
국어 컬럼 조회 결과

0    80
1    60
2    70
Name: 국어, dtype: int64

import pandas as pd


total_student  = int(input('총 성적의 개수 : '))
col_names = ['이름', '국어', '수학', '영어']


# 성적을 입력받고, 차례대로 컬럼에 넣어주는 함수 문법. 그러려니 
# Score 리스트  데이터 프레임으로 만들어준다
scores = [[int(col) if col.isdigit() else col for col in input().split()] for _ in range(total_student)]


df = pd.DataFrame(scores, columns=col_names)
print('\n', df, '\n')

# 지시사항을 수행해보세요.

#df 국어 컬럼을 조회해서 출력
print(df["국어"])


4. 데이터프레임에 새로운 컬럼 추가
데이터프레임에 값을 지정해주어 새로운 컬럼을 추가할 수 있습니다.


지시사항
학생의 이름, 국어, 수학, 영어 성적을 입력으로 받아 이를 저장한 데이터프레임 df 가 있습니다.

df에 입력된 학생 별 사회 성적이 담긴 new_score 리스트를 활용하여 사회 열을 추가한 후, df 를 출력해 보세요.

입력 예시

실행 후 Copy 버튼을 누르시면 아래 예시가 자동으로 입력됩니다.

3
홍길동 80 70 80
박철수 60 70 80
김영희 70 90 80
50 60 60
Copy
실행 결과

    이름  국어  수학  영어  사회
0  홍길동  80  70  80  50
1  박철수  60  70  80  60
2  김영희  70  90  80  60

import pandas as pd


total_student  = int(input('총 성적의 개수 : '))
col_names = ['이름', '국어', '수학', '영어']

#입력받아서 리스트로 만들어주는 문장
scores = [[int(col) if col.isdigit() else col for col in input().split()] for _ in range(total_student)]

# 입력받은 부분을 하나하나의 문자열로 바꿔주는 함수
new_score = list(map(int, input().split()))


df = pd.DataFrame(scores, columns=col_names)
print('\n', df, '\n')
print('new_score :', new_score, '\n')

# 지시사항을 수행해보세요.


# 새로운 컬럼을 추가하려면
# df["새로운 컬럼명"]=데이터(Series)

# '사회' 라는 성적은 new_score 변수에 담겨있습니다
# 사회 성적이 담긴 "사회" 라는 새로운 컬럼을 추가하고 출력
# df["새로운 컬럼명"]=데이터(Series)

df["사회"]=pd.Series(new_score)
print(df)



5.데이터프레임 숫자 세기
딕셔너리를 이용하면 데이터프레임의 특정 컬럼의 값을 변경할 수 있습니다.

숫자로 된 컬럼을 대상으로 데이터를 해석할 수 있도록 변환하고 몇 개로 구성되어 있는지 확인해보겠습니다.

[데이터 소개]
data.csv에는 성인 1만명의 건강검진 데이터이며 데이터는 다음과 같이 구성되어 있습니다.

컬럼 명	데이터 소개
BTH_G	연령(그룹)
SBP	수축기 혈압
DBP	연령(그룹)
FBS	공복 혈당
SEX	성별
DIS	고혈압/당뇨병 진료 여부
BMI	체질량 지수

지시사항
데이터프레임 df의 SEX 컬럼에 숫자로 된 데이터를 변환합니다.

1 → 남성
2 → 여성
데이터프레임 df의 DIS 컬럼에 숫자로 된 데이터를 변환합니다. (문자, 띄어쓰기가 안내된 사항과 다르면 채점이 정상적으로 작동하지 않습니다)

1 → 고혈압/당뇨병 진료내역 있음
2 → 고혈압 진료내역 있음
3 → 당뇨병 진료 내역 있음
4 → 고혈압/당뇨병 진료내역 없음
데이터프레임 df의 DIS 컬럼에 값들이 몇 개씩 저장되어 있는지 확인합니다.

실행 버튼을 눌러 올바르게 코드를 작성했는지 확인한 뒤, 제출 버튼을 눌러 점수를 확인하세요.

import pandas as pd

# 데이터프레임 df에 건강검진 데이터 저장
df = pd.read_csv('./data/data.csv')
print(df) # print 로 중간중간 데이터 값 확인 하기
# map함수 이용하여 딕셔너리 규격에 맞추어 변경.
# 원본에 붙여넣기 필요

# 지시사항1 (None을 지우고 알맞은 코드를 작성하세요)
sex = {
        1: "남성", 
        2: "여성"
      }

df['SEX'] = df['SEX'].map(sex)
print(df)

# 지시사항2 (None을 지우고 알맞은 코드를 작성하세요)
dis = {
        1: "고혈압/당뇨병 진료내역 있음",
        2: "고혈압 진료내역 있음", 
        3: "당뇨병 진료 내역 있음", 
        4: "고혈압/당뇨병 진료내역 없음"
       }

df['DIS'] = df['DIS'].map(dis)
print(df)

# 지시사항3 (None을 지우고 알맞은 코드를 작성하세요)
dis_count = df['DIS'].value_counts()

# 실행 버튼을 눌러 올바르게 지시사항을 수행했는지 확인해보세요
print(dis_count)


6. 콤마가 있는 숫자 처리하기
모든 데이터는 우리가 분석하기 편한 형태로 존재하고 있지 않습니다.

특히, 숫자 데이터는 콤마를 포함하는 경우가 있습니다.
ex) 1234 → 1,234

이럴 경우 컴퓨터는 콤마로 인해 문자열 자료형(object)로 인식하게 됩니다.

콤마가 있는 숫자에서 콤마를 제거하여 숫자형(int)으로 변경하는 실습을 진행하겠습니다.

[데이터 소개]
access_data.csv 에는 한 홈페이지의 접속경로 통계 데이터가 저장되어 있습니다.

컬럼 명	데이터 소개	데이터 유형
접속일자	접속한 날짜	object
PC접속수	PC를 통해 홈페이지에 접속한 수	object
모바일접속수	모바일을 통해 홈페이지에 접속한 수	object

지시사항
df(데이터프레임)의 ‘접속일자’ 컬럼의 자료형을 datetime으로 변경합니다.
del_comma 함수를 이용하여 ‘PC접속수’ 와 ‘모바일접속수’ 컬럼의 콤마를 제거합니다.
Tip del_comma 함수는 문자열에 콤마가 있다면 지우는 함수입니다.
콤마를 지운 ‘PC접속수’ 와 ‘모바일접속수’ 컬럼의 자료형을 숫자형 자료형(int)으로 변경합니다.
df의 ‘PC 접속수’ 컬럼과 ‘모바일접속수’ 컬럼을 이용해서 df에 ‘전체접속수’ 컬럼을 추가합니다.

import pandas as pd

df = pd.read_csv("./data/access_data.csv")

# 데이터프레임 정보 확인하기
print(df.info())
print("===================\n")

# 데이터프레임 내용 확인하기
print(df.head())


# 지시사항1 (None을 지우고 알맞은 코드를 작성하세요)
# 접속일자 컬럼 Datetime으로 변경
#df["접속일자"] = None   
df["접속일자"] = pd.to_datetime(df['접속일자'])
print(df.info) 


def del_comma(data: str):      # data: str : 문자열 연산을 했는데 정수형 데이터가 있으면 오류가 발생한다. str 문자열 자료형이다. 문자열 자료형만 적용되도록 함. 함수 내부에 사용할 자료형을 명시해놓은것
    "문자열 data에 콤마(,)가 있다면 제거하는 함수"
    if "," in data:
        data = data.replace(",", "")

    return data


# 지시사항2-1 (None을 지우고 알맞은 코드를 작성하세요)

# apply 매써드와 del_comma 함수를 이용하여 제거
#df["PC접속수"] = None
#df["모바일접속수"] = None

df["PC접속수"] = df["PC접속수"].apply(del_comma)
df["모바일접속수"] = df["모바일접속수"].apply(del_comma)

df["PC접속수"] = df["PC접속수"].apply(del_comma)
df["모바일접속수"] = df["모바일접속수"].apply(del_comma)


# 지시사항2-2 (None을 지우고 알맞은 코드를 작성하세요)
# 컬럼의 자료형을 int형으로 변경. astype()
# df["PC접속수"] = None
# df["모바일접속수"] = None

df["PC접속수"] = df["PC접속수"].astype(int)
df["모바일접속수"] = df["모바일접속수"].astype(int)





# 지시사항3 (None을 지우고 알맞은 코드를 작성하세요)

# df["전체접속수"] = None  # PC접속수 + 모바일 접속수
df["전체접속수"] = df["PC접속수"] + df["모바일접속수"]


# 실행 버튼을 눌러 올바르게 지시사항을 수행했는지 확인해보세요
print(df.info())
print("===================\n")

print(df.head())

7. 서울시 인구수 데이터 기반 고령화 정도 분석
2023년 서울시의 지역, 연령별 인구수 데이터를 활용하여 지역구별 고령화 정도를 분석해봅니다.

data 폴더에는 2023년 서울시 지역구별 인구수 데이터가 저장되어 있습니다.

서울시 인구수 데이터가 데이터프레임 형태로 불려와 df에 저장되어 있습니다.

아래 지시사항을 따라 데이터를 가공하는 수행하세요.


지시사항
데이터프레임 df에 “노인인구비율” 컬럼을 생성하여 지역구별로 총인구수 대비 노인인구의 비율을 저장합니다. (70대 이상의 인구를 노인 인구라고 가정합니다)

데이터프레임 df에 “고령여부” 컬럼을 생성하고 “노인인구비율” 컬럼값이 0.14 이상이면 O (대문자 o), 0.14 미만이면 X (대문자 x) 를 저장합니다.

실행 버튼을 눌러 올바르게 코드를 작성했는지 확인한 뒤, 제출 버튼을 눌러 점수를 확인하세요.

import pandas as pd


# 데이터프레임 df에 서울시 인구수 데이터 저장
df = pd.read_csv("./data/seoul_population.csv")

# 지시사항1 (None을 지우고 알맞은 코드를 작성하세요)
# df["노인인구비율"] = None
# df["노인인구비율"] = "70대 이상의 노인 인구 비율" / 전체 인구 대비

df["노인인구비율"] = (df["70대"]+df["80대"]+df["90대 이상"])/df["총인구수"]
print(df["노인인구비율"] )

# 지시사항2
# Hint: 함수를 정의해서 활용하시거나 람다 함수를 활용하세요

# def isDid(x):
#     x에 따른 return 값 정의

def isOld(x):
    if x >=0.14:
        return "0"
    else:
        return "X"

def isOld(x):
    if x >= 0.14:
        return "O"
    else:
        return "X"        

df["고령여부"]=df["노인인구비율"].apply(isOld)
df["고령여부"] = df["노인인구비율"].apply(isOld)

# 람다 함수 적용 예제

# df["고령여부"]=df["노인인구비율"].apply(lamda x : "0" if x>=0.14 else "X")

# 실행 버튼을 눌러 올바르게 지시사항을 수행했는지 확인해보세요
print(df)


8. 그룹으로 묶기
groupby() 함수를 이용하면 키 값을 기준으로 그룹으로 묶을 수 있습니다.

groupby() 함수의 사용 예시를 들자면, 시험 성적 데이터에서 각 반을 그룹으로 묶어 각 반의 평균 점수를 구할 수 있습니다.

예를 들어 아래와 같은 학생들의 성적을 나타낸 데이터 프레임이 있다고 생각해봅시다.

인덱스	반	국어성적	수학성적
0	A	100	85
1	B	96	88
2	C	94	91
3	A	92	94
4	B	90	97
5	C	86	100
이 때, 각 반별로 국어성적과 수학성적의 합을 구하고 싶을 수 있습니다.

같은 반 학생들의 값을 합하기 위해서 groupby() 함수를 사용할 수 있습니다.

group1 = df.groupby("반")
Copy
위 코드는 df를 "반"을 기준으로 묶는다는 의미입니다.

print(group1.sum())
Copy
그리고 group 객체의 sum 함수를 호출하면, 각 반별로 국어성적과 수학성적의 합을 구할 수 있습니다.


문제
Q1

주어진 데이터 프레임의 class칼럼을 기준으로 그룹을 만들어 합계를 구한 후 출력해보세요.

[출력 예시]

       Korean  Math
class              
A         192   179
B         186   185
C         180   191


import pandas as pd


def main():
    df = pd.DataFrame(
        {
            "class": ["A", "B", "C", "A", "B", "C"],
            "Korean": [100, 96, 94, 92, 90, 86],
            "Math": [85, 88, 91, 94, 97, 100],
        }
    )
    print("DataFrame:")
    print(df, "\n")

    # groupby 함수를 이용해봅시다.
    # class를 기준으로 묶어 합계를 구해보세요.
    


if __name__ == "__main__":
    main()

##########################################################################################################################################################
##########################################################################################################################################################

##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################

#############################################################################
2강 시작
#############################################################################


##########################################################################################################################################################

이전 시간 배웠던 내용들
1. 판다스 불러오기
import pandas as pd 

2. 자료 불러오기
pd.read_csv("자료위치")  

3. 요약정보 불러오기
df.head()  df.tail()   df.info()



# 데이터 추출하기

## 데이터 불러오기

import pandas as pd

df=pd.read_csv("./data/seoul_park03.csv")
mm=pd.read_csv("./data/misemunji.csv")



## 조건에 따른 인덱싱: Boolean indexing

전체 데이터프레임에서 Boolean indexing을 통해 특정 조건에 맞는 데이터를 추출할 수 있습니다. 예를 들어 어른 입장객 수가 5000보다 큰 날짜들의 데이터를 추출하려면 다음과 같이 코드를 작성할 수 있습니다.

# 어른 column에서 최대값과 두번째로 큰 값을 구하기
max_adult = df['어른'].max()
second_max_adult = df['어른'].nlargest(2).iloc[-1]

# 최대값과 두번째로 큰 값의 날짜 구하기
max_date = df[df['어른'] == max_adult]['날짜'].values[0]
second_max_date = df[df['어른'] == second_max_adult]['날짜'].values[0]

print("어른의 최대값: ", max_adult)
print("두번째로 큰 값: ", second_max_adult)
print("어른의 최대값인 날짜: ", max_date)
print("두번째로 큰 값인 날짜: ", second_max_date)

from datetime import datetime

# 두 날짜간의 차이 구하기
date_format = '%Y-%m-%d'
max_date_obj = datetime.strptime(max_date, date_format)
second_max_date_obj = datetime.strptime(second_max_date, date_format)

date_diff = abs((max_date_obj - second_max_date_obj).days)

print("두 날짜간의 차이: {}일".format(date_diff))

import numpy as np

# 어른의 최대값과 두번째로 큰 값
x = np.array([max_adult, second_max_adult])

# 해당 값들이 있는 인덱스
y = np.array([0, 1])

# 1차 방정식 구하기
coefficients = np.polyfit(x, y, 1)
a = coefficients[0]
b = coefficients[1]

print("1차 방정식: y = {}x + {}".format(a, b))

import numpy as np

# 어른의 최대값과 두번째로 큰 값
x = np.array([max_adult, second_max_adult])

# 해당 값들이 있는 인덱스
y = np.array([0, 1])

# 1차 방정식 구하기
coefficients = np.polyfit(x, y, 1)
a = coefficients[0]

print("1차 방정식의 기울기: {}".format(a))

df[df["어른"]>5000]
# 서울 대공원 입장객 데이터
#df[조건식]

Pandas 논리연산자를 활용하면 좀 더 복잡하게 조건을 설정할 수 있습니다. 두 개 이상의 조건을 and(`&`)나 or(`|`)을 활용하여 같이 사용하는데, 이 때 각 조건식은 소괄호로 묶여있어야 합니다. 

어른 입장객의 수가 10000명이 넘는 공휴일의 데이터만을 추출하려면 and(`&`) 연산자를 활용해 다음과 같이 추출할 수 있습니다.

# amd 연산자 어른이 10000 초과고 공휴일은 O(알파벳)인 경우만
df[(df["어른"]>10000) & (df["공휴일"]=="O")]

이번에는 or(`|`)연산자를 활용해 어른 입장객의 수가 10000명이 넘거나 어린이 입장객 수가 2000명이 넘는 날짜의 데이터를 추출해보도록 하겠습니다.

# OR 연산자. 둘중 하나만 만족  shift + \ = |
df[(df["어른"]>10000) | (df["어린이"]>2000)]

### [TODO] 어린이날의 데이터들을 추출해 변수 `childrensday`에 저장하세요.
* 어린이날은 5월 5일입니다. "월"과 "일" 컬럼을 사용하세요.
* Pandas 논리연산자를 사용할 때에는 각 조건을 소괄호로 묶어주어야 합니다.

# None을 지우고 알맞은 코드를 입력하세요.
# childrensday=None
childrensday=df[(df["월"]==5) & (df["일"]==5)]
# 데이터가 알맞게 추출되었는지 확인합니다.
childrensday

## 라벨을 활용한 데이터 추출: loc

다시 데이터를 살펴보겠습니다.

df.head()

우리가 일반적으로 데이터프레임에서 원하는 데이터를 추출한다고 가정해봅시다. 가령 2016년 1월 4일의 어른 입장객 데이터를 알고싶은 경우, 데이터프레임을 쭉 보면서 2016년 1월 4일의 행을 찾고, 그 행에서 "어른" 열을 찾아 그 값을 확인합니다.

`loc`은 이러한 데이터 탐색 및 추출과정을 활용한 데이터 추출 메서드입니다. **입력받은 데이터의 행과 열의 인덱스를 활용하여 그 위치에 해당하는 데이터를 추출**합니다. 

여기서 유의할 점은 우리가 사용하고있는 데이터의 행 인덱스는 날짜가 아니라 인덱스 숫자입니다. 2016년 1월 4일 데이터의 인덱스값은 3이므로, `loc`을 사용하여 2016년 1월 4일의 어른 데이터를 추출하려면 다음과 같이 사용할 수 있습니다.

# 위 데이터는 인덱스 이름이 0부터 순서대로 되어 있다
# loc의 인덱스 이름과 iloc의 행번호가 똑같다는 것을 의미한다
df.loc[3,'어른']

연속적인 객체(데이터프레임의 인덱스) 범위를 지정해 가져오는 방법인 슬라이싱을 활용하면 범위를 지정하여 해당 범위에 해당하는 데이터들을 불러올 수도 있습니다.

이 때 유의할 점은, `loc`은 라벨 기반 인덱싱을 사용하기 때문에 **`A:B`로 슬라이싱을 하면 A 부터 B까지, 즉 B포함한 범위를 인덱싱 한다는 점**입니다. 예를들어 `3:6` 의 범위를 지정한다면 인덱스가 3부터 6까지인 데이터, `"어른":"외국인"` 의 범위를 지정한다면 "어른"부터 "외국인" 까지의 데이터를 지정하게 됩니다. 

또한 이렇게 슬라이싱을 활용해 추출한 데이터들은 복수의 데이터이므로, 시리즈 혹은 데이터프레임 형태라는 사실을 알아두면 좋습니다.

# 슬라이싱할 때  첫번째: 마지막: step
# step은 생략시 1로 디폴트, -1로 하면 거꾸로 인덱싱 된다, 
# 거꾸로 하려면 인덱싱 시작지점을 반대로 적어줘야 한다 순서가 거꾸로기 때문
# step이 2면 간격을 가지고 추출한다
  
df.loc[3:6,"어른":"외국인"]

# df.loc[3:6,"외국인":"어른":-1]   
# 스텝 역순으로 하기 위해 시작-끝 인덱스 순서 바꿔줘야함

# df.loc[3:6,"어른":"외국인":2]

## loc과 Boolean indexing을 활용한 데이터 추출

앞서 배웠던 Boolean indexing처럼 조건식과 논리연산자를 loc과 같이 활용하면 조건에 맞는 데이터들만을 추출할 수 있습니다. 예를 들어 다음과 같이 어른과 어린이의 입장객 수가 둘다 1000보다 큰 날짜의 "날짜"부터 "총계" 행을 추출할 수 있습니다.

# ,콤마를 기준으로 왼쪽은 행 오른쪽은 열
df.loc[(df['어른']>1000) & (df['어린이']>1000), "날짜":"총계"]

## 순서를 활용한 데이터 추출: iloc

다음으로 행과 열의 정수 위치를 이용해 데이터를 추출하는 `iloc`에 대해 알아보겠습니다. 앞서 `loc`이 행과 열의 이름을 좌표로 삼아 해당 위치의 데이터를 추출했다면, `iloc`은 **행과 열의 정수형 위치, 즉 순서를 좌표로 삼아 해당 위치의 데이터를 추출**합니다. 예를 들어 4번 행(2016년 1월 5일)의 7번 열(외국인) 데이터를 추출하면 다음과 같습니다.

# iloc는 위치정보이기 떄문에 정수만 들어간다
df.iloc[4, 7]

`iloc` 역시 슬라이싱을 활용하여 지정한 범위의 데이터를 추출할 수 있습니다. 

여기서 `loc`과의 중요한 차이점이 있는데, `iloc`은 위치 기반 인덱싱을 사용하여 범위를 지정하기 때문에 시작은 포함되고 끝은 포함되지 않습니다. 즉 `iloc`에서 **`A:B`로 슬라이싱을 하면 A부터 B-1까지, 즉 B를 포함하지 않는 범위를 인덱싱**합니다. B를 포함해서 인덱싱하는 `loc`과는 다르기 때문에 코드를 작성하거나 해석할 때 유의하셔야 합니다.

`iloc`을 활용해서 인덱싱을 3&#126;6번 행, 4&#126;6번 열의 값을 추출하면 다음과 같습니다.

df.iloc[3:7, 4:7]    # loc는 뒤에 인덱싱 포함, iloc는 뒤에 인덱싱 포함 안함

# 스텝 -1 적용
#df.iloc[7:3:-1, 7:4:-1] 

`iloc`은 정수 위치를 사용하기 때문에 단순한 작업보다는, for문 등을 활용한 반복작업시에 매우 유용하게 활용할 수 있습니다.

`loc`과 `iloc`을 활용하면 특정 위치에 해당하는 데이터 값을 추출할 수도 있고, 그 값을 다른 값으로 바꿔넣을 수도 있습니다. 예를 들어 2016년 1월 5일의 "청소년" 컬럼 값을 확인해보겠습니다.

df.loc[4, "청소년"]

만약 이 값을 다른 값으로 바꿔주고 싶다면 `loc`이나 `iloc`을 활용해 값을 불러온 다음, 바꿔줄 값을 `=`을 활용해 저장해주면 됩니다.

df.loc[4, "청소년"] = 100   # 4번 인덱스의 청소년값에 100을 삽입해준다
df.head()

2016년 1월 5일의 "청소년" 컬럼의 값이 100으로 바뀐 것을 확인할 수 있습니다.

### [TODO] 2018년 전체의 날짜, 미세먼지, 초미세먼지 데이터를  변수 `dust`에 저장하세요.
* 미세먼지 데이터 `mm`에서 2018년 1월 1일의 행 인덱스는 **731**, 2018년 12월 31일의 행 인덱스는 **1095** 입니다.
* `loc` 또는 `iloc`을 활용해 2018년의 **"날짜"와 "미세먼지", "초미세먼지"** 컬럼 값을 추출해 데이터프레임으로 저장하세요.

# 미세먼지 데이터
mm.head()

# loc 또는 iloc을 활용해 지정된 데이터를 dust에 저장합니다.
# None을 지우고 알맞은 코드를 입력하세요.
# dust=None
dust=mm.loc[731:1095, "날짜":"오존"]
# 초미세먼지 수치가 제대로 추출되었는지 확인합니다.
dust

## 채점
* **[TODO]** 중 수행하지 않은 부분이 없는지 확인하세요.
* 채점을 위해 아래 코드를 실행한 뒤 우측 상단의 제출 버튼을 눌러주세요.
* 코드 수정시 정상적인 채점이 이루어지지 않습니다.

# loc 또는 iloc을 활용해 지정된 데이터를 dust에 저장합니다.

# None을 지우고 알맞은 코드를 입력하세요.

date = pd.to_datetime(mm[“날짜”])
year = date.dt.year

dust=mm.loc[(year == 2018), “날짜”:”초미세먼지”]

# 초미세먼지 수치가 제대로 추출되었는지 확인합니다.




mm[“날짜”]=pd.to_datetime(mm[“날짜”])
mm[“날짜”>2016-05-05 ]

이런 형식으로 가능하게 하려면 어떻게 해야 하나요?

날짜를 스트링으로 인식후에, 처음 4개의 스트링이 2018인 것만 추출하면 안될까


import pandas as pd

# "날짜" 열을 datetime 형식으로 변환
mm["날짜"] = pd.to_datetime(mm["날짜"])

# 2016-05-05 이후의 날짜를 가진 행 선택
selected_rows = mm[mm["날짜"] > "2016-05-05"]
print(selected_rows)

max 말고 max 다음으로 큰 2번째 3번째를 볼수 있는 그런 함수도 있을까요?

sort로 정렬후에 n번째를 선택

import os
import json

childrensday.to_json("problem_1.json")
dust.to_json("problem_2.json")

result = {}
result["problem_1"] = "problem_1.json"
result["problem_2"] = "problem_2.json"

with open("result.json", "w") as f:
    json.dump(result, f)
#######################################################################################################################################################################################################################################



##########################################################################################################################################################

##########################################################################################################################################################


##########################################################################################################################################################


#######################################################################################################################################################################################################################################



#################  2강 실습 문제 ####################################################################################################

1. 조건에 맞는 데이터 조회
행과 열에 대한 정보를 통해 단 하나의 데이터를 조회할 수 있다면 조건을 통해 조건을 만족하는 모든 데이터를 조회할 수도 있습니다.


지시사항
학생의 이름, 국어, 수학, 영어 성적을 저장한 데이터프레임 객체 df 에서 국어 점수가 70점 이상인 데이터만 조회해서 출력해 보세요.

입력 예시

3
홍길동 80 70 80
박철수 60 70 80
김영희 70 90 80
Copy
실행 결과

-- 국어 점수가 70점 이상인 데이터 --
    이름  국어  수학  영어
0  홍길동  80  70  80
2  김영희  70  90  80


import pandas as pd

# Copy를 통해 값을 입력받아서 데이터 프레임으로 만들어주는 문장
total_student  = int(input('총 성적의 개수 : '))
col_names = ['이름', '국어', '수학', '영어']
scores = [[int(col) if col.isdigit() else col for col in input().split()] for _ in range(total_student)]
df = pd.DataFrame(scores, columns=col_names)
print('\n', df, '\n')

print('-- 국어 점수가 70점 이상인 데이터 --')

# 지시사항을 수행해보세요.

#국어 점수가 70점 이상인 데이터만 조회해서 출력
print(df[df['국어']>=70])


2. 여러 조건에 맞는 데이터 조회
데이터프레임에서 데이터를 조회할 때 한 가지 조건만이 아닌 다양한 조건을 결합해서 조회할 수도 있습니다.


지시사항
학생의 이름, 국어, 수학, 영어 성적을 저장한 데이터프레임 객체 df 에서 국어 점수가 70점 이상이고 수학 점수가 80점 미만인 데이터를 조회해서 출력해 보세요.

입력 예시

3
홍길동 80 70 80
박철수 60 70 80
김영희 70 90 80
Copy
실행 결과

-- 출력된 데이터 --
    이름  국어  수학  영어
0  홍길동  80  70  80

import pandas as pd


total_student  = int(input('총 성적의 개수 : '))
col_names = ['이름', '국어', '수학', '영어']
scores = [[int(col) if col.isdigit() else col for col in input().split()] for _ in range(total_student)]
df = pd.DataFrame(scores, columns=col_names)
print('\n', df, '\n')

print('-- 출력된 데이터 --')

# 지시사항을 수행해보세요.
# 국어 점수가 70점 이상이고(그리고) 수학 점수가 80점 미만인 데이터를 조회해서

print(df[(df['국어']>=70) & (df['수학']<80)])


3. 데이터 프레임 슬라이싱 (loc)
loc를 통해 명시적인 인덱스를 참조하는 데이터 프레임의 슬라이싱이 가능합니다. (명시적 인덱싱)

아래와 같은 데이터프레임이 있다고 가정해봅시다.

gdp	인구	gdp_per_pop
china	1409250000	141500	9959.363957597174
japan	516700000	12718	40627.457147350215
korea	169320000	5180	32687.258687258687
usa	2041280000	32676	62470.31460399069
print(country.loc["japan":"usa"])
Copy
위와 같이 loc로 "japan":"usa"와 같이 슬라이싱한다면, japan, korea, usa 인덱스가 출력됩니다.

슬라이싱의 결과
loc로 슬라이싱을 한 결과는 DataFrame 데이터로 반환됩니다.


지시사항
명시적 인덱스를 사용하여 데이터 프레임에서 "japan" 인덱스부터 "usa" 인덱스 까지 출력해보세요. (["japan":"usa"]로 슬라이싱합니다.)

import pandas as pd

# 첫번째 칼럼을 인덱스로 country.csv 파일 읽어오기.
print("Country DataFrame")

country = pd.read_csv("data/country.csv", index_col=0)
print(country, "\n")

# "japan" 인덱스부터 "usa" 인덱스까지 출력해봅시다.
print(country.loc["japan":"usa"]) 
# 행만 적고 열은 생략 ,: 전체 컬럼 뽑아짐. 컬럼은 생략해도 됨 print(country.loc["japan":"usa",:]) 
print(country.loc["japan":"usa",:]) 


4. 데이터 프레임 슬라이싱 (iloc)
마찬가지로 iloc으로도 데이터 프레임을 슬라이싱 하실 수 있습니다.

아래와 같은 데이터프레임이 있다고 가정해봅시다.

gdp	인구	gdp_per_pop
china	1409250000	141500	9959.363957597174
japan	516700000	12718	40627.457147350215
korea	169320000	5180	32687.258687258687
usa	2041280000	32676	62470.31460399069
print(country.iloc[0:2])
Copy
iloc으로 [0:2]로 슬라이싱해주면 0, 1번째 인덱스들이 출력됩니다.

슬라이싱의 결과
iloc으로 슬라이싱을 한 결과는 DataFrame 데이터로 반환됩니다.


지시사항
정수 인덱스를 사용하여 데이터 프레임에서 0번 인덱스부터 1번 인덱스까지의 값들을 출력해보세요. ([0:2]로 슬라이싱합니다.)

import pandas as pd

# 첫번째 칼럼을 인덱스로 country.csv 파일 읽어오기.
print("Country DataFrame")

country = pd.read_csv("data/country.csv", index_col=0)
print(country, "\n")

# 0번 인덱스부터 1번 인덱스까지 출력해봅시다.
print(country.iloc[0:2])


5. 다이어트 실험 조건 분석
exercise.csv 파일에는 다이어트 중 지방 섭취 양, 행동 시간, 검사 중 상황에 따른 심박수를 조사한 데이터가 저장되어 있습니다. 지방 섭취 양이나 행동 시간 등에서 특정 조건과 일치하는 실험의 결과를 한눈에 파악하려고 합니다.

exercise.csv 파일 내의 컬럼에 대한 설명은 다음과 같습니다.

컬럼 명	형식	내용
id	숫자	검사 대상 인원
diet	문자	지방 섭취 양
pulse	숫자	심박수
time	문자	행동 시간
kind	문자	검사 중 상황
지시사항을 참고하여 코드를 작성하세요.


지시사항
exercise.csv 파일을 pandas 라이브러리를 이용하여 불러옵니다.

첫 번째 줄에 조건으로 비교할 컬럼 명을 입력받습니다.

문자 형식으로 이루어진 컬럼만을 비교 대상으로 선택합니다.
두 번째 줄에 내용과 비교할 문자열을 입력받습니다.

불러온 csv 파일을 이용하여 첫 번째 줄에 입력받은 컬럼의 내용이 두 번째 줄에 입력받은 문자열과 일치하는 행 전체를 출력합니다.

상위 20줄까지 출력합니다.
입력 예시

diet
no fat
Copy
출력 예시
출력 형식이 다른 경우 오답 처리될 수 있어 형식에 유의해주세요.

    id    diet  pulse    time     kind
15   6  no fat     83   1 min     rest
16   6  no fat     83  15 min     rest
17   6  no fat     84  30 min     rest
18   7  no fat     87   1 min     rest
19   7  no fat     88  15 min     rest
20   7  no fat     90  30 min     rest
21   8  no fat     92   1 min     rest
22   8  no fat     94  15 min     rest
23   8  no fat     95  30 min     rest
24   9  no fat     97   1 min     rest
25   9  no fat     99  15 min     rest
26   9  no fat     96  30 min     rest
27  10  no fat    100   1 min     rest
28  10  no fat     97  15 min     rest
29  10  no fat    100  30 min     rest
45  16  no fat     84   1 min  walking
46  16  no fat     86  15 min  walking
47  16  no fat     89  30 min  walking
48  17  no fat    103   1 min  walking
49  17  no fat    109  15 min  walking
Copy
Tips!
시험 환경에서 실행 단추를 눌렀을 때 오류가 출력되는 경우 0점 처리됩니다.
입력 예시가 동작하지 않는 경우 채점이 정상적으로 진행되지 않을 수 있습니다.
출력 예시와 출력 형식이 다르거나 다른 내용이 추가로 출력되는 경우 오답 처리될 수 있습니다.
input() 함수의 괄호 내부는 완전히 비워주세요.
O: input()
X: input("숫자를 입력하세요: ")
X: input("")


import pandas as pd

# 지시사항을 참고하여 코드를 작성하세요.
# read_csv 이용하여 exercise.csv 파일 불러오기

df=pd.read_csv("exercise.csv")
print(df)
#조건으로 비교할 컬럼 명  diet 컬럼
index = input()

#비교할 문자열  no fat 값
content = input()

# 첫번째 줄에 입력받은 컬럼의 내용이 두 번째 줄에 입력 받은 문자열과 일치하는 행 전체를 출력합니다.
#print(df[df[index]==content])

# 상위 20줄까지 출력 합니다.

print(df[df[index]==content].head(20))   # df[df['diet']=='no fat'].head(20)

# print( df[df[“diet”] == “no fat”].head(20) )


6. 독버섯일 확률 알아보기
버섯의 분류 특성에 대한 데이터를 이용하여 버섯의 갓 모양과 색상에 따라 독버섯일 확률을 구해보려고 합니다.

mushrooms.csv 파일에 버섯의 분류 특성에 대한 데이터가 담겨 있습니다.

해당 CSV파일의 칼럼은 각각 다음의 의미가 있습니다.

칼럼	의미	예시
class	독버섯 여부	edible
cap-shape	갓의 모양	convex
cap-color	갓의 색상	white
odor	냄새	almond
population	개체 수	scattered
habitat	서식지	meadows
모든 칼럼의 데이터는 문자열 형태입니다.
각 칼럼에 대해 가능한 값은 아래의 표를 참고하세요.

칼럼	가능한 값
class	edible, poisonous
cap-shape	bell, conical, convex, flat, knobbed, sunken
cap-color	brown, buff, cinnamon, gray, green, pink, purple, red, white, yellow
odor	almond, anise, creosote, fishy, foul, musty, none, pungent, spicy
population	abundant, clustered, numerous, scattered, several, solitary
habitat	grasses, leaves, meadows, paths, urban, waste, woods
지시사항을 참고하여 코드를 작성하세요.


지시사항
함수 ratio()를 아래 내용을 참고하여 완성하세요.

매개변수	설명 (형태)
df	버섯 특성 데이터 (데이터프레임)
shape	버섯 갓의 모양 (문자열)
color	버섯 갓의 색상 (문자열)
버섯 갓의 모양이 shape이고, 버섯 갓의 색상이 color인 버섯들 중 한 종류를 골랐을 때 독버섯일 확률을 백분율로 구하여 반환합니다.

값은 소수점 아래 자리를 버린 정수 형태여야 합니다.
반환되는 형태는 정수형이어야 합니다.
함수 호출 예시

print(ratio(df, "convex", "red"))
Copy
출력 예시

61
Copy
Tips!
기본적으로 주어진 함수를 삭제하거나 지시사항에서 제시한 이름을 사용하지 않으면 채점 시 오류가 발생할 수 있으니 유의하시기 바랍니다.


# 아래 코드는 문제 해결을 위해 기본적으로 제공되는 코드입니다. 수정하지 마세요!
import pandas as pd

df = pd.read_csv("mushrooms.csv")
print(df)
# 지시사항을 참고하여 코드를 작성하세요.
# 매개변수 : 함수 외부에서 값을 받아서 함수 내부에서 사용되는 함수. ( df, shape, color ) 변수들이 괄호안에 들어간다
# 마지막줄 프린트 문에 df, convex, red가 들어간다
def ratio(df, shape, color):
    
    # df에서 버섯 갓의 모양이 `shape`이고(and), 버섯 갓의 색상이 `color`인 버섯들을 고르는 코드를 작성하세요
    total = df[(df['cap-shape']==shape)&(df['cap-color']==color)]
    
    
    # total에서 class를 확인하여 독이 있는 버섯만 저장하는 코드를 작성하세요 
    poison = total[total["class"]=="poisonous"]
    
    #len : 행의 개수
    return int(100 * len(poison) / len(total))



# 결과를 확인하기 위한 코드입니다. 값을 변경해가며 테스트해 보세요!
print(ratio(df, "convex", "red"))


7. 수면에 관한 설문 데이터 분석하기
수면에 관한 설문 결과 데이터를 가지고 한 사람과 비슷한 수면 습관을 지니는 사람들을 찾아 분석해보려고 합니다.

sleep.csv 파일에 수면 설문 조사 결과 데이터가 담겨 있습니다.

인덱스는 설문에 응한 사람들의 이름으로 되어 있으며, 이름은 중복 데이터가 존재하지 않습니다.

해당 CSV파일의 칼럼은 각각 다음의 의미가 있습니다.

칼럼	의미	예시
Enough	충분한 수면을 했는지	No
Hours	평균적인 수면 시간	5
PhoneReach	손이 닿을 거리에 핸드폰을 두고 자는지	Yes
PhoneTime	잠들기 30분 전에 핸드폰을 사용하는지	Yes
Tired	하루 동안 어느 정도 피곤한지	2
Breakfast	보통 아침을 먹는지	No
Enough, PhoneReach, PhoneTime, Breakfast 칼럼의 값은 Yes와 No만 존재합니다.
Hours는 2부터 10까지의 정수, Tired는 1부터 5까지의 정수로 이루어져 있습니다. Tired의 경우 1로 갈수록 피곤하지 않다는 의미이고, 5로 갈수록 피곤하다는 의미입니다.
지시사항을 참고하여 코드를 작성하세요.


지시사항
함수 sleep()을 아래 내용을 참고하여 완성하세요.

매개변수는 다음과 같습니다.
매개변수	설명
df	수면 설문 조사 결과 데이터 (데이터프레임)
name	기준이 될 설문 응답자의 이름 (문자열)
주어진 df에서 인덱스가 name에 해당하는 데이터를 찾습니다.
해당 데이터와 PhoneTime, Tired, Breakfast 값이 일치하는 데이터를 추출합니다. (인덱스가 name인 본인 데이터도 포함됩니다.)
추출한 데이터에서 Enough 값(Yes 혹은 No)에 대해 Hours 평균을 구한 후 그 값 사이의 차를 구합니다. (절댓값 abs() 함수를 사용하세요.)
반환되는 형태는 실수형이어야 합니다.
함수 호출 예시

print(sleep(df, "Wendy"))
Copy
출력 예시

아래의 예시는 문제를 푸는 데 참고할 수 있는 예시입니다. 최종적으로 제출해야 하는 답안과는 관계가 없습니다. 출력 예시를 그대로 반환하면 오답 처리됩니다.

1.0
Copy
Tips!
기본적으로 주어진 함수를 삭제하거나 지시사항에서 제시한 이름을 사용하지 않으면 채점 시 오류가 발생할 수 있으니 유의하시기 바랍니다.

# 아래 코드는 문제 해결을 위해 기본적으로 제공되는 코드입니다. 수정하지 마세요!
import pandas as pd

df = pd.read_csv("sleep.csv", index_col=0)
print(df)
# 지시사항을 참고하여 코드를 작성하세요.
def sleep(df, name):
    
    # 주어진 `df`에서 인덱스가 `name`에 해당하는 데이터를 찾는 코드를 작성하세요
    person = df.loc[name]  #person에  name = wendy라는 친구의 데이터가 담기게 된다
    print(person)
    
    # person에서 `PhoneTime`, `Tired`, `Breakfast` 값이 일치하는 데이터를 추출하는 코드를 작성하세요
    # 인덱스가 `name`인 본인 데이터도 포함됩니다
    p = person['PhoneTime'] # wendy의 PhoneTime : No
    t = person['Tired'] # wendy의 Tired정도 : 3
    b = person['Breakfast'] # wendy의 Breakfast 여부 : Yes


#전체 사람들중에 웬디랑 똑같은 사람을 추출해보자
    people = df[(df['PhoneTime']==p) & (df['Tired']==t) & (df['Breakfast']==b)]  #(p=No, t=3, b=Yes)
    
    # 추출한 데이터(people)에서 `Enough` 값(`Yes` 혹은 `No`)에 대해 `Hours` 평균을 구한 후 그 값 사이의 차를 구합니다.
    yes = people[people['Enough']=='Yes']['Hours'].mean()   #Enough yes에 대한  평균값

# Enough가 yes 에 따른 hours 의 평균을 구하여라. 였다면, 아래와 같이 groupby를 사용하면 될까요? 24/03/27 15:50 녹화내용 10분쯤부터 보기
# --> 조건이 Yes or No 하나면 그룹바이 쓰고 이번건 아니라서 각각 구해준것 Yes/ No


    

    no = people[people['Enough']=='No']['Hours'].mean()   #Enough no에 대한 평균값

    print(people["PhoneTime"])
    #전체 데이터 프레임에서 데이터 줄이고,결과 값이 데이터 프레임이라면 괄호를 더 붙여서 범위를 좁혀나갈수 있다
    # [조건검색][다른 column].함수  조건 검색후에 데이터 프레임 형식이면 가능하다 
    print(people[people["Enough"]=="No"]["Hours"])
    result = yes-no
    
    
    
    return abs(result)


# 결과를 확인하기 위한 코드입니다.
print(sleep(df, "Wendy"))



8. 수컷 펭귄 분석하기
데이터를 활용하여 수컷 펭귄 중 부리의 길이가 입력값보다 큰 펭귄들의 부리의 깊이의 평균을 알아보려 합니다.

penguins.csv 파일에 펭귄에 대한 정보가 담겨 있습니다.

*데이터가 존재하지 않는 행은 사용하지 않습니다.

해당 CSV 파일의 칼럼은 각각 다음을 의미합니다.

칼럼	의미	예시
species	펭귄의 ‘종’	Adelie
island	섬 이름	Torgersen
bill_length_mm	부리의 길이 ( 단위 : mm)	39.1
bill_depth_mm	부리의 깊이 ( 단위 : mm)	18.7
flipper_length_mm	날개의 길이 ( 단위 : mm)	181
body_mass_g	체질량 ( 단위 : g)	3750
sex	성별 ( MALE, FEMALE)	MALE
지시사항을 참고하여 코드를 작성하세요.


지시사항
함수 total()을(를) 아래 내용을 참고하여 완성하세요.

데이터가 없는 행은 제외하고 데이터를 추출합니다.
데이터프레임 형태의 펭귄 데이터와 정수인 
�
n(단위 : mm)을 매개변수로 받습니다.
‘sex’ 가 ‘MALE’이고, 부리의 길이가 
�
n보다 큰 펭귄의 수를 구해 answer의 원소로 추가하고, 해당 펭귄들의 부리의 깊이 평균을 구하여 answer 리스트의 원소로 추가하여 출력하세요.
평균은 pandas에서 제공하는 mean()을 사용합니다.
함수 반환값은 리스트형태 입니다.
함수 호출 예시

print(total(df, 30))
Copy
출력 예시

아래의 예시는 문제를 푸는데 참고할 수 있는 예시입니다. 최종적으로 제출해야 하는 답안과는 관계가 없습니다. 출력 예시를 그대로 반환하면 오답 처리됩니다.

[168, 17.89107142857143]
Copy
Tips!

# 아래 코드는 문제 해결을 위해 기본적으로 제공되는 코드입니다. 수정하지 마세요!
import pandas as pd

df = pd.read_csv("penguins.csv")
print(df)
# 지시사항을 참고하여 코드를 작성하세요.
def total(df, s):
    answer = []

    # (수정 금지) 데이터가 없는 행은 제외하는 코드입니다
    # dropna : Nan 데이터 날리는 역할 
    df1 = df.dropna()
    
    
    # 'sex' 가 'MALE'이고, 'bill_length_mm'가 s보다 큰 펭귄을 구하는 코드를 작성하세요
    df1 = df1[(df1['sex']=="MALE")&(df['bill_length_mm']>s)]
    
    
    # df1의 길이를 구하고 `answer`의 원소로 추가하는 코드를 작성하세요.
    #None
    # df1의 길이를 구하면 len 펭귄의 수가 구해진다. len함수는 행의 갯수를 카운팅한다
    answer.append(len(df1))
    print(answer)
    # 해당 펭귄들(df1)의 'bill_depth_mm' 평균을 구하여 `answer` 리스트의 원소로 추가하는 코드를 작성하세요
    # 평균은 pandas에서 제공하는 `mean()`을 사용하세요
    answer.append(df1['bill_depth_mm'].mean())
    
    
    return answer


# 값을 확인하기 위한 코드입니다. 값을 변경해가며 테스트해 보세요!
print(total(df, 30))



9. 펭귄 종이 서식하고 있는 섬의 개수
데이터를 활용하여 펭귄의 특정 ‘종’(species)이 몇 종류의 섬에 서식하는지 알아보려 합니다.

penguins.csv 파일에 펭귄에 대한 정보가 담겨 있습니다.

해당 CSV 파일의 칼럼은 각각 다음을 의미합니다.

칼럼	의미	예시
species	펭귄의 ‘종’	Adelie
island	섬 이름	Torgersen
bill_length_mm	부리의 길이 ( 단위 : mm)	39.1
bill_depth_mm	부리의 깊이 ( 단위 : mm)	18.7
flipper_length_mm	날개의 길이 ( 단위 : mm)	181
body_mass_g	체질량 ( 단위 : g)	3750
sex	성별 ( MALE, FEMALE)	MALE
지시사항을 참고하여 코드를 작성하세요.


지시사항
함수 total()을(를) 아래 내용을 참고하여 완성하세요.

데이터프레임 형태의 펭귄 데이터와 ‘종’의 이름을 매개변수로 받습니다.
해당 ‘종’이 살고 있는 섬의 종류의 수를 출력합니다.
함수 반환값은 정수입니다.
함수 호출 예시

print(total(df, "Adelie"))
Copy
출력 예시

아래의 예시는 문제를 푸는데 참고할 수 있는 예시입니다. 최종적으로 제출해야 하는 답안과는 관계가 없습니다. 출력 예시를 그대로 반환하면 오답 처리됩니다.

3
Copy
Tips!

# 아래 코드는 문제 해결을 위해 기본적으로 제공되는 코드입니다. 수정하지 마세요!
import pandas as pd

df = pd.read_csv("penguins.csv")
print(df)
# 지시사항을 참고하여 코드를 작성하세요.
# df = pd.read_csv("penguins.csv")
# s : 종의 이름(문자열)
def total(df, s):

    # 'df' 중에서 'species' 열의 값이 인자 's'와 일치하는 행들만을 필터링하여 'df1'에 저장하는 코드를 작성해주세요
    df1 = df[df['species']==s]
    # df1 = df[df['species']=="Adelie"]

    # 'df1' 에서 'island' 열의 값들을 추출하는 코드를 작성해주세요
    # set()은 집합 자료형으로써 중복을 제거하기 위해 사용하는 코드입니다  set(1 1 2 2 3 3 )-> (1 2 3)
    df1 = set(df1['island'])
    #df1 = df1['island']  # set 중복 제거 안하면 많아진다
    # df1 species : Adelie인것만 남긴다음, 섬의 종류를 중복 제거 날렸다. 그럼 남은 행의 갯수는 Adelie이면서 섬의 갯수 중복 아니니깐 행의 갯수가 결국 Adelie가 사는 섬의 갯수가 된다

    return len(df1)


# 값을 확인하기 위한 코드입니다. 값을 변경해가며 테스트해 보세요!
print(total(df, "Adelie"))




10. 맨해튼 내에서만 일어난 운행
taxis.csv 파일에 위의 정보가 담겨있습니다.

해당 CSV 파일의 칼럼은 각각 다음을 의미합니다.

칼럼	예시
pickup	2019-03-23 20:21:09
dropoff	2019-03-23 20:27:24
passengers	1
distance	1.6
fare	7.0
tip	2.15
tolls	0.0
total	12.95
color	yellow
payment	credit card
pickup_zone	Lenox Hill West
dropoff_zone	UN/Turtle Bay South
pickup_borough	Manhattan
dropoff_borough	Manhattan
지시사항을 참고하여 코드를 작성하세요.


지시사항
taxis.csv를 데이터프레임 형태로 저장한 info가 주어집니다.

new_info에서 pickup_borough와 dropoff_borough 칼럼의 값이 모두 “Manhattan”인 행만 new_info에 저장합니다.
new_info는 데이터프레임입니다.
지시사항 1의 new_info에서 payment에 따른 distance의 평균을 cal에 저장하세요.
cal은 pd.Series 입니다.

import pandas as pd
import numpy as np

info = pd.read_csv("taxis.csv")

# (수정 금지) 데이터가 없는 행을 제외하는 코드입니다 
new_info = info.dropna()



# `new_info`에서 `pickup_borough`와 `dropoff_borough` 칼럼의 값이 모두 "Manhattan"인 행만 `new_info`에 저장하는 코드를 작성하세요
new_info = new_info[(new_info['pickup_borough']=='Manhattan')&(new_info['dropoff_borough']=='Manhattan')]



# ~에 따른 groupby 함수 사용하면된다
# mean()
# `new_info`에서 `payment`에 따른 `distance`의 평균을 `cal`에 저장하는 코드를 작성하세요(시리즈 형태로 반환해야함)
cal = new_info.groupby('payment')['distance'].mean()
print(cal)


# 아래의 코드는 정답과 관계가 없는 코드입니다. 변경을 자유롭게 해주셔도 됩니다.
print(new_info)
print(cal)


#############################################################################

##########################################################################################################################################################

3강 시작
#############################################################################

##########################################################################################################################################################

# 데이터 정제하기

## 데이터 불러오기

결측치(missing value) : 값이 없는 것
1. NaN  : Not a Number 숫자가 아니다  -> 결측치를 통용해서 쓰이기도 한다 본 예문에서
2. Null : 아무것도 없다
3. Na : Not available  유효하지 않다

import pandas as pd

df=pd.read_csv("./data/seoul_park03.csv")
mm=pd.read_csv("./data/misemunji.csv")

df.head()

이번에는 데이터를 다양하게 정제해서 사용하는 방법에 대해 알아보겠습니다.

## 데이터 정렬: sort_values

먼저 데이터를 정렬해서 사용하는 방법에 대해 알아보겠습니다. 현재의 데이터는 시간의 순서대로 기록되어 있는데, 특정 상황에서는 입장객 수에 따라서 정렬을 하면 사용이 용이한 경우가 많습니다. 

이럴 때 우리는 `sort_values()`를 사용할 수 있습니다. `sort_values()`는 특정 컬럼의 값을 기준으로 전체 데이터를 오름차순 혹은 내림차순으로 정렬합니다. 입장객 수가 많은 순서를 알아보기 위해 "총계" 컬럼을 기준으로 내림차순으로 정렬해보도록 하겠습니다.

# 데이터를 확인하기 위해 정렬한 데이터프레임을 df_sorted에 저장
df_sorted=df.sort_values("총계", ascending=False)
df.sort_values("총계",ascending=False) #내림차순
# 입장객수가 많았던 날짜의 데이터 상위 10개 출력
df_sorted.head(10)

입장객 수가 많았던 순서대로 정렬해보니 데이터가 집계된 기간의 **어린이날**들이 모두 최상위권에 위치해 있어 어린이날의 위엄을 알 수 있습니다. 이는 단순히 어린이날이라는 이유 뿐 만 아니라, 서울대공원의 경우 어린이날 당일에는 13세 미만 어린이들에게 무료 개방을 하고 있기 때문입니다. 이러한 정보를 통해 어린이날의 "어린이" 컬럼의 값이 비정상적으로 낮고 "무료합계" 컬럼이 높은 이유 또한 알 수 있습니다.

또한 전부 공휴일인 것은 모두가 예상하신대로 일 것이고, 어린이날을 제외한 상위권을 살펴보면 **2016년 6월 5일은 다음날이 공휴일인 일요일, 2017년 4월 8일과 9일은 서울대공원 벚꽃축제, 2017년 4월 23일은 서울대공원 겹벚꽃 원더풀 데이 축제 날짜**였습니다. 이렇게 특정 이벤트가 있는 날짜의 입장객 수가 평범한 날들에 비해 월등히 높은 것을 알 수 있습니다.

이번에는 입장객 수 하위 10개 데이터를 살펴보겠습니다.

df_sorted.tail(10)

하위 2개 데이터는 폐쇄 후 재개장을 위한 테스트로 추정되기 때문에 제외하고, 나머지 데이터를 살펴보면 일단 전부 공휴일이 아니라는 것을 알 수 있습니다. 최하위권에 **2018년 1월 24일부터 25일까지**가 위치해있는데, 날씨가 맑음임에도 불구하고 아무리 평일이라도 입장객 수가 지나치게 낮은 것을 확인할 수 있습니다. 이유를 알아보면 해당 일차의 날씨 자체는 맑음이 맞지만 **강렬한 한파로 인해 서울의 최저기온이 영하 18도까지 내려갔던 기간**이었습니다.

이렇게 데이터를 정렬하면 추가 정보를 얻을 수 있고, 데이터 분석의 방향을 결정할 수도 있습니다.

### [TODO] 미세먼지 데이터에서 초미세먼지 농도가 3번째로 심했던 날짜를 변수 `pollution`에 저장하세요.
* `mm`에는 미세먼지 데이터가 데이터프레임 형태로 저장되어있습니다.
* `sort_values`를 활용해 "초미세먼지" 컬럼을 기준으로 내림차순 정렬하고, 3번째 데이터를 확인합니다.
* 초미세먼지 농도가 3번째로 높았던 날짜를 `"yyyymmdd"` 형식의 문자열로 변수 `pollution`에 저장하세요. (ex. 2016년 1월 5일 -> `pollution="20160105"`)

# sort_values를 활용해 데이터프레임 mm을 "초미세먼지" 컬럼 기준으로 내림차순 정렬하세요.
mm.sort_values("초미세먼지",ascending=False)

# 초미세먼지 농도가 3번째로 높은 날의 날짜정보를 변수 pollution에 저장하세요.
# None을 지우고 알맞은 값을 입력하세요.
pollution="20190304"
print(pollution)

## 인덱스 재지정: reset_index

이런식으로 데이터를 정렬하여 데이터를 사용하다보면 한 가지 문제가 발생합니다. 다시 `df_sorted`를 살펴보시면 데이터는 "총계"컬럼에 맞춰서 정렬되었지만 이 과정에서 기존의 인덱스를 그대로 가지고 정렬된 사실을 확인할 수 있습니다. 이렇게 될 경우 앞서 배웠던 `loc` 등을 활용하기가 어렵기 때문에, 이 데이터를 가지고 계속 데이터 분석을 할 예정이라면 인덱스를 재정렬해줄 필요가 있습니다. 이 때 우리는 `reset_index()`를 활용합니다.

df_sorted.reset_index(drop=True) # drop을 True로 지정해주어 "index"컬럼 생성 안함. True 지워주면 

`df_sorted`의 인덱스가 깔끔하게 재정렬된 것을 확인할 수 있습니다. 이렇게 인덱스를 재지정은 지금처럼 데이터를 정렬하거나, 일부 데이터를 삭제하는 등의 작업을 통해 인덱스가 흐트러졌을 때 수행해주면 유용한 작업입니다.

## 데이터 삭제: drop

데이터에는 다양한 정보가 저장되어 있지만, 데이터분석 목적에 따라 크게 중요하지 않은 정보들이 있을 수 있습니다. 가령 서울대공원 데이터를 다시 살펴보면 "유료합계" 컬럼과 "무료합계" 컬럼은 입장객 수를 분석하는 데에 있어서 크게 중요하지 않습니다. 이런 경우 해당 컬럼을 `drop()`을 활용하여 삭제하여 좀 더 깔끔하고 명확한 데이터를 분석하는데 활용할 수 있습니다. "유료합계"와 "무료합계" 컬럼을 삭제하여 `df`에 덮어씌우는 과정은 다음과 같습니다.

# 삭제할 2개의 컬럼을 리스트로 입력
# axis=1로 설정하여 컬럼을 삭제
df.drop(["유료합계", "무료합계"], axis=1)


# 행을 삭제하려면 행번호 입력
# df.drop([1,2,3],axis=0)
# df.drop(df[df["날씨"]=="구름많음"].index,axis=0)
# df = df[df["날씨"] != "구름 많음"] #하면 되긴함
df

필요 없는 컬럼이 삭제되어 좀더 깔끔한 데이터가 된 것을 확인할 수 있습니다. 

## 열 이름 바꾸기: rename

떄로는 데이터의 컬럼이름이 잘못 설정되어있거나, 알아보기 어렵게 설정되어있는 경우가 있습니다. 이런 경우 `rename()`을 활용해 컬럼의 이름을 바꿀 수 있습니다. 서울대공원 데이터에서 "총계" 컬럼의 경우 총 관람객 수를 의미한다는 것을 유추할 수 있지만, 좀 더 명확하게 "총계" 컬럼의 이름을 "총입장객수" 로 바꿔보도록 하겠습니다.

# .columns 는 모든 컬럼을 바꿔줘야한다

df=df.rename(columns = {"총계": "총입장객수"})   #원하는 컬럼만 바꿀수 있다
df

컬럼의 이름이 "총입장객수" 로 바뀐것을 확인할 수 있습니다. 

## 결측치 처리

우리가 실습에서 사용하고 있는 데이터는 실제 서울대공원의 입장객 수를 기록한 데이터입니다. 그런데 실제로 데이터를 수집하다보면 다양한 문제가 야기됩니다. 앞서 처리했던 표기의 문제도 있었고, 데이터 자료형의 문제도 있었습니다. 그리고 또 다른 문제로는 데이터의 누락이 있습니다. 오랜 기간 데이터를 수집하다보면 데이터가 수집되지 않는 경우들이 발생할 수 있습니다.


`info()`를 활용하여 다시 한번 데이터를 살펴보겠습니다.

df.info()
# null이 아닌 항목의 개수
# Non-Null Count

# 1086
# Non-Null Count 카운트가 946 -> Null 140

데이터의 갯수를 보면 대부분의 컬럼이 1086개의 데이터를 가지고 있지만 "날씨" 컬럼과 "청소년" 컬럼의 데이터가 부족한것을 확인할 수 있습니다. 잘보면 데이터의 갯수가 Non-Null Count라고 되어있는데, 즉 "날씨" 컬럼과 "청소년" 컬럼에는 Null이 존재한다는 뜻입니다.

이 Null이 어떻게 생겼는지는 데이터프레임에서 확인할 수 있습니다.

df.head()

2016년 1월 4일의 데이터의 "청소년" 컬럼을 확인하면 NaN(Not a Number)라고 되어있습니다. 이는 0이 아니라 아예 공백, 즉 수집되지 않아서 어떤 값인지 알 수 없는 데이터입니다. 이런 값들을 **결측치** 라고 합니다.

당연한 얘기지만, 이런 결측치들은 데이터 분석이나 인공지능 적용 과정에서 걸림돌로 작용합니다. 이제부터 이 결측치를 처리하는 방법을 배워보겠습니다.

## 결측치 탐색: isnull()

먼저 결측치가 얼마나 존재하는지 알아보기 위해 `isnull()`을 활용합니다. 물론 앞서 봤듯이 `info()`를 활용해서도 알 수 있지만, `isnull()`을 활용하면 추가적인 계산 필요 없이 직관적으로 어떤 컬럼에 몇 개의 결측치가 존재하는지 확인할 수 있습니다. 먼저 데이터프레임에 `isnull()`을 적용해보겠습니다.

df.isnull()  #True가 나오면 그 행은 결측치라는 것을 의미합니다

`isnull()`은 데이터프레임의 각 원소가 결측치인지 아닌지를 검사합니다. 결측치가 아닌 제대로된 값은 False, 결측치는 True로 채워진것을 확인할 수 있습니다. 하지만 저 표를 가지고는 결측치가 컬럼별로 몇개인지 확인할 수가 없습니다. 이 때 True는 1로, False는 0으로 계산되어 `sum()`을 활용해서 각 컬럼별로 전체값의 합을 구하면 결과값은 컬럼에 존재하는 True의 갯수, 즉 결측치의 갯수가 됩니다.

df.isnull().sum()

# True : 정수1
# False : 정수0
# 결측치 몇개인지 확인한다

"날씨" 컬럼에 140개, "청소년" 컬럼에 5개의 결측치가 있는 것을 알 수 있습니다. 이렇게 결측치의 갯수를 파악하는 것은 결측치를 어떻게 처리할지 방법을 결정하는데에 중요합니다.

결측치를 처리하는 방법은 대표적으로 2가지가 있습니다. 하나는 결측치를 **특정 값으로 채워넣는 것**이고, 하나는 **결측치가 존재하는 데이터를 삭제하는 것**입니다. 

## 결측치 채우기: fillna()

먼저 결측치를 채워넣는 방법에 대해 알아보겠습니다. `fillna()`는 결측치를 특정 값으로 채워넣는데 활용하는 메서드로, 어떤 값으로 채워넣는지는 다양한 방법을 사용합니다. 이번 실습에서는 "청소년" 컬럼의 결측치를 전체 데이터의 청소년 입장객의 평균값으로 채워보도록 하겠습니다.

#df.fillna(int(df["청소년"].mean()))  # df만 있으면 전체 데이터에 적용된다


df["청소년"].fillna(int(df["청소년"].mean()))

원래는 결측치였던 2016년 1월 4일의 청소년 데이터가 463명이라는 펑균값으로 채워진것을 확인할 수 있습니다. 물론 이 값이 정확한 값이라고는 할 수 없지만, 최대한 확률이 높은 값으로 채워넣었다고 생각하면 됩니다.

## 결측치 데이터 삭제하기: dropna()

결측치가 많지 않을 경우, 결측치가 있는 데이터를 삭제해버리는 것도 좋은 방법입니다. 가령 청소년 컬럼에서 결측치가 있는 데이터를 삭제하면 5일치의 데이터가 사라지는 것입니다. 이는 전체 기간 1086일의 데이터에서 비중이 크지 않기 때문에 괜찮은 방법이 될 수도 있습니다. 이럴 때 `dropna()`를 활용하면 알아서 기준 컬럼에서 결측치를 탐사하고 해당하는 데이터를 삭제합니다.

df.dropna(subset="청소년") # 삭제하고자하는 컬럼이름 저장

이번에는 "청소년" 컬럼에 결측치가 있었던 2016년 1월 4일의 데이터가 아예 삭제된 것을 확인할 수 있습니다. 

이렇게 결측치를 처리하는 방법은 다양하지만, 이번 강의에서는 이후 강의진행의 편의성을 위해 결측치를 채워넣는 방식으로 `df`를 변형하도록 하겠습니다.

# df=df.fillna(int(df["청소년"].mean()))  # 전체 데이터 프레임에 적용돼서 안된다
df["청소년"]=df["청소년"].fillna(int(df["청소년"].mean()))  # 청소년 컬럼에만 결측치 찾아서 거기만 채워줌
df.tail(20)

### [TODO] 미세먼지 데이터에서 결측치가 있는 컬럼을 파악하고 해당 컬럼의 중앙값으로 채운 뒤 `mm`에 덮어씌워 저장하세요.
* 미세먼지 데이터가 담겨있는 데이터프레임 `mm`의 한 컬럼에 결측치가 4개 존재합니다. 
* 먼저 `isnull()`과 `sum()`을 활용하여 어떤 컬럼에 결측치가 존재하는지 확인하세요.
* 결측치를 **결측치가 있는 컬럼 전체의 중앙값**으로 채운 뒤, `mm`의 해당 컬럼에 덮어씌워 저장하세요. 중간값 집계함수는 `median()`입니다.
* 모든 지시사항을 수행한 뒤 데이터프레임 `mm`에는 결측치가 없어야 합니다.

# 미세먼지 데이터를 다시 불러와 변수 mm에 저장합니다.
mm=pd.read_csv("./data/misemunji.csv")
mm.head()


# isnull()과 sum()을 활용해 어떤 컬럼에 결측치가 존재하는지 확인하세요.
mm.isnull().sum()

#isna() 해당 컬럼에 결측치가 있다면, 그행을 출력합니다
# 결측치가 있는 컬럼만 불러올 수 있다
mm[mm["이산화질소"].isna()]  

# 전체 결측치가 어떻게 되어있는지 보고싶다. 모든 컬럼의 결측값이 있는 행을 다 불러옴
mm[mm.isna().any(axis=1)]

# mm의 결측치를 지시사항에 맞게 채워넣은 뒤 덮어씌우세요.
mm["이산화질소"]=mm["이산화질소"].fillna(mm["이산화질소"].median())  
# int 는 정수형이므로 소수점에서는 안써도 됨 . median이 숫자 처리해줌




## 채점
* **[TODO]** 중 수행하지 않은 부분이 없는지 확인하세요.
* 채점을 위해 아래 코드를 실행한 뒤 우측 상단의 제출 버튼을 눌러주세요.
* 코드 수정시 정상적인 채점이 이루어지지 않습니다.

import os
import json

mm.to_json("problem_2.json")

result = {}
result["problem_1"] = pollution
result["problem_2"] = "problem_2.json"

with open("result.json", "w") as f:
    json.dump(result, f)

##########################################################################################################################################################


##########################################################################################################################################################



##########################################################################################################################################################



##########################################################################################################################################################


# 데이터 병합하기

데이터 병합은 여러 개의 데이터셋을 하나로 합치는 과정을 말합니다. 여러 개의 데이터셋이 각각 다른 정보를 담고 있을 때 이 데이터들을 합치면 데이터의 크기를 늘리거나 종합적인 정보를 얻을 수 있습니다. 

## 데이터 불러오기

import pandas as pd

df=pd.read_csv("./data/seoul_park04.csv")
df.head()



## 데이터 병합: concat()

먼저 데이터 확장을 위해 기존의 서울대공원 입장객 데이터에 이후 기간 데이터를 병합하는 과정을 알아보도록 하겠습니다.

df.tail()

실습에 활용하고있는 서울대공원 입장객 데이터는 2019년 3월 31일까지의 데이터가 담겨 있습니다. 이 데이터의 뒤에 2019년 4월 한달간의 데이터를 불러와서 합치면 2016년 1월 1일부터 2019년 4월 30일까지의 데이터를 만들 수 있습니다.

df2=pd.read_csv("./data/seoul_park_april.csv")
df2.head()

`concat()`을 활용하면 두 개 이상의 데이터프레임을 행 또는 열 방향으로 단순히 이어붙이는데 활용합니다. 지금의 경우 2019년 3월 31일까지의 데이터인 `df`의 아래방향으로 2019년 4월의 데이터 `df2`를 이어붙이면 되기 때문에 `concat()`을 활용합니다.

아래 방향으로 붙이기 때문에 `axis`를 **0**으로, 두 데이터프레임에 모두 존재하는 컬럼만을 남기기 위해 `join`을 **inner**로, 인덱스를 전체 초기화하기 위해 `ignore_index`를 **True**로 설정합니다.

pd.concat([df, df2], axis=0, join='inner', ignore_index=True)  # axis=0 기본값으로 생략 가능

`concat()`을 활용하여 2016년 1월 1일부터 2019년 4월 30일까지의 데이터를 만들었습니다. 이런식으로 데이터를 늘리면 더 긴 기간동안의 데이터를 분석할 수 있고 데이터 분석 결과의 정확도를 높일 수 있습니다.

## 데이터 병합: merge()

데이터 병합은 정보의 종류를 늘리는데도 활용할 수 있습니다. 서울대공원 데이터셋과 미세먼지 데이터셋을 합치면, 날씨 뿐 만 아니라 미세먼지 농도에 따른 입장객 수의 정보를 확인할 수도 있습니다. 서울대공원 데이터셋과 같은 기간동안 수집된 미세먼지 데이터를 불러오도록 하겠습니다.

mm=pd.read_csv("./data/misemunji.csv")
mm.head()

 **2016년 1월 1일부터 2019년 3월 31일까지의 서울대공원 입장객 데이터**에 **2016년 1월 1일부터 2019년 3월 31일까지의 미세먼지 데이터**를 합쳐보도록 하겠습니다. 
 
`merge()`를 활용해서 데이터를 합칠 때에는 기준이 될 컬럼이 필요합니다. 이번 경우에는 날짜를 기준으로 입장객수와 미세먼지 데이터간의 관계를 확인하기 위해 기준 컬럼인 `on`을 **날짜**로 하겠습니다.

`how`는 데이터를 합치는 방법을 지정합니다. 데이터를 합치는 방법은 데이터 분석의 목적에 따라 달라지므로 각 방법을 선택하는 이유에 대한 이해가 필요합니다.

우리의 관심사는 미세먼지에 따른 입장객의 수이기 때문에, 입장객 수 데이터가 없는 부분은 필요하지 않습니다. 두 데이터의 수집 기간은 같지만, 서울대공원 입장객 데이터의 경우 조류독감으로 인해 폐쇄된 구간의 데이터가 없기 때문에 해당 기간의 미세먼지 데이터는 필요가 없습니다. 따라서 입장객 수의 데이터가 존재하는 구간의 데이터만을 얻기 위해 `how`를 입장객 수 데이터, **left**에 맞춘 것입니다. (이번 실습의 경우 **inner**를 활용해도 같은 결과를 얻을 수 있습니다)

pd.merge(df, mm, on = '날짜', how = 'left') # 데이터프레임1, df2, 컬럼명 기준치 , how 

이렇게 두 데이터를 날짜를 기준으로 합쳐서 더욱 더 다양한 정보가 담긴 데이터를 만들었습니다. 만약 이 데이터를 활용해서 데이터 분석이나 그래프를 그리는 시각화 등을 수행한다면 입장객 수와 미세먼지 농도의 관계를 파악할 수 있습니다.

## [TODO] 다양한 데이터 병합 실습을 수행합니다.

# 실습을 위한 데이터를 생성합니다.
score = pd.DataFrame({
    '이름': ['민수', '도윤', '서아', '연우', '하준'],
    '국어': [95, 72, 83, 89, 100],
    '수학': [88, 72, 93, 80, 95],
    '영어': [96, 68, 85, 94, 93]},
)

add1 = pd.DataFrame({
    '이름': ['지호', '준우', '예서'],
    '국어': [88, 92, 100],
    '수학': [97, 91, 100],
    '영어': [87, 95, 97],
    '역사': [95, 100, 92]}
)

add2 = pd.DataFrame({
    '이름': ['민수', '서아', '하준'],
    '과학': [93, 82, 89],
    '프로그래밍': [97, 96, 94]}
)

`score`에는 기존의 특별반 5명 학생들의 **이름**과 **국어, 수학, 영어** 점수가 담겨있습니다.
| | 이름  | 국어  | 수학  | 영어  |
|---|---|---|---|---|
| 0 | 민수  | 95  | 88  | 96  |
| 1 | 도윤  | 72  | 72  | 68  |
| 2 | 서아  | 83  | 93  | 85  |
| 3 | 연우  | 89  | 80  | 94  |
| 4 | 하준  | 100 | 95  | 93  |

`add1`에는 3명의 새로운 특별반 학생들의 **이름, 국어, 수학, 영어, 역사**점수가 기록되어 있습니다.

| | 이름  | 국어  | 수학  | 영어  | 역사 |
|---|---|---|---|---|---|
| 0 | 지호  | 88  | 97  | 87  | 95 |
| 1 | 준우  | 92  | 91  | 95  | 100 |
| 2 | 예서  | 100  | 100  | 97  | 92 |

`add2`에는 기존의 특별반 5명의 학생들 중 **과학**과 **프로그래밍**시험을 본 학생들의 시험점수가 **이름**과 함께 기록되어있습니다. 

| | 이름  | 과학  | 프로그래밍 |
|---|---|---|---|
| 0 | 민수  | 93  | 97  |
| 1 | 서아  | 82  | 96  |
| 2 | 하준  | 89  | 94  |

### 1. `score`과 `add1`을 활용해 전체 특별반 8명 학생들의 **국어, 수학, 영어** 점수데이터를 만들어 `ans1`에 저장하세요.
* Hint: `pd.concat()`을 활용하세요.
* `ans1`에는 다음과 같은 데이터프레임이 저장되어야 합니다.
| | 이름  | 국어  | 수학  | 영어  |
|---|---|---|---|---|
| 0 | 민수  | 95  | 88  | 96  |
| 1 | 도윤  | 72  | 72  | 68  |
| 2 | 서아  | 83  | 93  | 85  |
| 3 | 연우  | 89  | 80  | 94  |
| 4 | 하준  | 100 | 95  | 93  |
| 5 | 지호  | 88  | 97  | 87  |
| 6 | 준우  | 92  | 91  | 95  |
| 7 | 예서  | 100  | 100  | 97  |

# None을 지우고 알맞은 코드를 입력하세요.
# 행을 기준으로 병합하기 때문에, concat함수 사용
# axis=0 생략 가능
ans1=pd.concat([score, add1], axis=0, join='inner', ignore_index=True)  # join = outer 로하면 역사도 추가, 인덱스 초기화 위해 True
ans1

### 2. `score`과 `add2`을 활용해 기존 특별반 학생들 중 **국어, 수학, 영어, 과학, 프로그래밍** 5개 과목의 시험을 모두 본 학생들의 점수데이터를 만들어 `ans2`에 저장하세요. 
* Hint: `pd.merge()`를 활용하세요.
* `ans2`에는 다음과 같은 데이터프레임이 저장되어야 합니다.
| | 이름  | 국어  | 수학  | 영어  | 과학  | 프로그래밍 |
|--|---|---|---|---|---|---|
| 0 | 민수  | 95  | 88  | 96  | 93  | 97  |
| 1 |서아  | 83  | 93  | 85  | 82  | 96  |
| 2 |하준  | 100 | 95  | 93  | 89  | 94  |

# None을 지우고 알맞은 코드를 입력하세요.
# 가로로 이어야해서 컬럼기준으로 합치기 위해 merge 사용
ans2=pd.merge(score, add2, on = '이름', how = 'right')  # 교집합 how = inner 로하면 교집합이니깐 동일하게 나옴
ans2

## 채점
* **[TODO]** 중 수행하지 않은 부분이 없는지 확인하세요.
* 채점을 위해 아래 코드를 실행한 뒤 우측 상단의 제출 버튼을 눌러주세요.
* 코드 수정시 정상적인 채점이 이루어지지 않습니다.

import os
import json

ans1.to_json("problem_1.json")
ans2.to_json("problem_2.json")

result = {}
result["problem_1"] = "problem_1.json"
result["problem_2"] = "problem_2.json"

with open("result.json", "w") as f:
    json.dump(result, f)
##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################

##########################################################################################################################################################

##########################################################################################################################################################

##########################################################################################################################################################

##########################################################################################################################################################

#######################################################################################################################################################################################################################################

##########################################################################################################################################################





##########################################################################################################################################################


##########################################################################################################################################################


##########################################################################################################################################################


#############################################################################


#################  3강 실습문제 ####################################################################################################################################

1. 꽃 종류 정렬 (2)
csv 파일을 기준에 따라 정렬하고 특정 범위를 출력하여 확인하려고 합니다.

지시사항을 참고하여 코드를 작성하세요.


지시사항
iris.csv 파일을 pandas 라이브러리를 이용하여 불러옵니다. 파일 내의 컬럼에 대한 설명은 다음과 같습니다.

컬럼 명	형식	내용
sepal_length	숫자	꽃받침 길이
sepal_width	숫자	꽃받침 너비
petal_length	숫자	꽃잎 길이
petal_width	숫자	꽃잎 너비
species	문자	꽃의 종류
첫 번째 줄에 출력을 시작할 행 번호를 입력받습니다.

두 번째 줄에 출력을 종료할 행 번호를 입력받습니다.

세 번째 줄에 정렬할 기준의 컬럼 명을 입력받습니다.

불러온 csv 파일을 이용하여 세 번째 줄에 입력받은 컬럼 명을 기준으로 하여 오름차순으로 정렬합니다.

정렬한 데이터를 첫 번째 줄에 입력받은 행 번호부터 두 번째 줄에 입력받은 행 번호까지 출력합니다.

import pandas as pd

# 지시사항을 참고하여 코드를 작성하세요.
df=pd.read_csv("iris.csv")

#입력받는 열
start = int(input())
end = int(input())
col_name = input()

# col_name을 기준으로 오름차순 정렬, ascending=True 생략 가능
df = df.sort_values(col_name, ascending=True)

#입력받은 행~열 까지
# 인덱스 번호가 1번부터 진행되어 있고
# 행번호는 0부터 시작하기 때문에 iloc도 가능
# print(df.iloc[start:end]) 
print(df.iloc[start:end])


2. 쑥쑥 자라라 콩나무야!
“쑥쑥 자라라 콩나무야!”

잭은 여러 개의 신비한 힘을 지닌 콩을 심고 하늘까지 쑥쑥 자라는 콩나무의 모습을 보았어요.

쑥쑥 자란 콩나무들의 키, 둘레, 열린 콩의 개수 데이터가 csv파일로 만들어져 있네요.

우리는 csv 데이터를 읽고, 정렬해서 height가 큰 순서대로 상위 5개 나무의 정보를 확인해 보고자 해요!

result


지시사항
height 변수를 큰 순서대로 정렬하고, 큰 순서대로 상위 5개의 나무를 출력하세요.

height가 큰 나무 5개를 출력할 때엔 인덱싱 을 활용해주세요. 혹은, 아래 Tips와 같이 head 함수를 활용하여도 동일한 결과를 확인하실 수 있습니다.

출력 예시
출력 예시로, 실제 출력 결과와 다른 값이 출력됩니다. 출력 형식만 참고하세요.


import pandas as pd

tree_df = pd.read_csv("./data/tree_data.csv")
# 지시사항을 참고하여 코드를 작성하세요.

tree_df = tree_df.sort_values("height", ascending=False)
# 큰수부터 출력
#내림차순 정렬 큰순서대로 볼수 있음
print(tree_df.head(5))





3. 서울시 지역구별 혼인이혼 통계 데이터 병합
2023년 서울시의 지역, 연령별 인구수 데이터와 혼인/이혼 건수 데이터에서 특정 데이터를 골라서 수정하고 병합하여 활용합니다.

data 폴더에는 2023년 서울시 지역구별 인구수 데이터와 함께 지역구별 혼인 및 이혼 건수 데이터가 저장되어 있습니다.

데이터프레임 df1에 서울시 지역구/연령별 인구수 데이터가, df2에 서울시 지역구/연령별 혼인 및 이혼 건수 데이터를 불러와 저장되어 있습니다.

아래 지시사항을 따라 데이터를 가공하고 병합하는 과정을 수행하세요.


지시사항
서울시 혼인 및 이혼 건수 데이터에 잘못된 값이 저장되어 있었습니다. loc 또는 iloc을 활용하여 df2의 마포구의 혼인 건수를 1520으로 변경하세요.

서울시 인구수 데이터와 혼인/이혼 데이터를 하나의 데이터로 합치려합니다. 지역구를 기준으로 두 데이터를 합쳐 ans_df에 저장하세요. (행 및 열의 순서는 자유롭게 설정하세요)

실행 버튼을 눌러 올바르게 코드를 작성했는지 확인한 뒤, 제출 버튼을 눌러 점수를 확인하세요.


import pandas as pd

# 데이터프레임 df1에 서울시 인구수 데이터, df2에 서울시 혼인/이혼 데이터 저장
df1 = pd.read_csv("./data/seoul_population.csv")
df2 = pd.read_csv("./data/seoul_marriage_divorce_data.csv")

print(df2)
# 지시사항1
# Hint: 좌측 상단 구석의 파일 트리를 열고 혼인-이혼 데이터를 확인하세요. 행 인덱스는 0부터 시작해야 합니다.
# df2 의 마포구 혼인건수를 1520으로 수정
df2.loc[10, "혼인"]=1520


# 지시사항2 (None을 지우고 알맞은 코드를 작성하세요.)
# "지역구" 컬럼이 중복, 지역구 컬럼을 기준으로 삼아 병합
ans_df=pd.merge(df1,df2,on="지역구", how="inner")

# 올바르게 지시사항을 수행했는지 확인해보세요
print(ans_df)




4. 허리둘레 변환하기
옷을 살 때와 같이 허리둘레를 cm 단위가 아닌 inch 단위로 알고 있어야 하는 일들이 자주 있습니다.

데이터를 활용하여 건강보험 가입자의 허리둘레를 inch로 알아보고자 합니다.

waist.csv 파일에 허리둘레(단위 : cm)에 대한 정보가 담겨 있습니다.

해당 CSV 파일의 칼럼은 각각 다음을 의미합니다.

칼럼	예시
가입자 일련번호	1
성별코드	1
허리둘레	72.1
여기서 허리둘레의 단위는 cm이며 성별코드 
1
1은 남성, 
2
2는 여성을 나타냅니다.

지시사항을 참고하여 코드를 작성하세요.

*: 본 저작물은 ‘국민건강보험공단’에서 ‘21년’작성하여 이용허락범위 제한 없음으로 개방한 ‘국민건강보험공단_건강검진정보_20211229(작성자:빅데이터운영실 데이터관리1팀)’을 이용하였으며,
해당 저작물은 ‘공공데이터포털,https://www.data.go.kr/data/15007122/fileData.do 무료로 다운받으실 수 있습니다.


지시사항
waist.csv를 데이터프레임 형태로 저장한 info가 주어집니다.

주어진 info에서 데이터가 누락된 행은 삭제합니다.
허리둘레(inch) column을 생성하고 모든 행의 허리둘레(inch) 칼럼은 아래의 식을 이용해 변환한 값의 소수 첫째 자리를 버림하여 정수로 저장합니다.

변환 식은 다음과 같습니다.

허리둘레(inch)=허리둘레/2.54

기존의 모든 칼럼을 포함해야 합니다.
info_column_add은 데이터프레임입니다.



import numpy as np
import pandas as pd

info = pd.read_csv("waist.csv")

# 지시사항 1번을 참고하여 코드를 작성하세요.
print(info)
# 주어진 `info`에서 데이터가 누락된 행은 삭제하는 코드를 작성하세요
info = info.dropna()


# (수정 금지) 데이터프레임에 '허리둘레(inch)'라는 새로운 열을 추가하고, 이 열의 모든 값들을 'NaN' (Not a Number, 즉 결측값)으로 설정하는 코드입니다
info["허리둘레(inch)"] = np.nan



# 지시사항을 참고하여 허리둘레를 inch 단위로 변환하는 식을 적용하는 코드를 작성하세요 
info["허리둘레(inch)"] = info["허리둘레(inch)"].fillna(info["허리둘레"]/2.54)
print(info)


# 변환된 허리둘레를 정수형(소수 첫째 자리 버림)으로 변환하는 코드를 작성하세요
info["허리둘레(inch)"] = info["허리둘레(inch)"].astype(int)



info_column_add = info

print(info_column_add)


5. 부상자수 구하기
accident.csv 파일에 가해운전자의 연령층 및 월별 교통사고 통계 정보가 담겨있습니다.

해당 CSV 파일의 칼럼은 각각 다음을 의미합니다.

칼럼	예시
가해자연령층	20세이하
발생월	01
사고건수	363
사망자수	4
중상자수	95
경상자수	368
부상신고자수	46
지시사항을 참고하여 코드를 작성하세요.


지시사항
accident.csv를 데이터프레임 형태로 저장한 info가 주어집니다.

주어진 info에서 데이터가 누락된 행은 삭제합니다.
부상자수 칼럼을 생성하고 모든 행의 부상자수 값을 정수형으로 채우세요.

부상자수는 중상자수, 경상자수 그리고 부상 신고자수를 더한 값입니다.

기존의 모든 칼럼을 포함해야 합니다.
info_column_add은 데이터프레임입니다.

# 아래 코드는 문제 해결을 위해 기본적으로 제공되는 코드입니다. 수정하지 마세요!
import numpy as np
import pandas as pd

info = pd.read_csv("accident.csv")

# 지시사항을 참고하여 코드를 작성하세요.


# 주어진 `info`에서 데이터가 누락된 행은 삭제하는 코드를 작성하세요
info = info.dropna()



# (수정 금지) 데이터프레임에 '부상자수'라는 새로운 열을 추가하고, 이 열의 모든 값들을 'NaN' (Not a Number, 즉 결측값)으로 설정하는 코드입니다
info["부상자수"] = np.nan

print(info)

# 모든 행의 `부상자수` 값을 정수형으로 채우는 코드를 작성하세요
# 부상자수는 중상자수, 경상자수 그리고 부상 신고자수를 더한 값입니다.
info["부상자수"] = info["부상자수"].fillna(info["중상자수"]+info["경상자수"]+info["부상신고자수"]) # 중상자수, 경상자수 그리고 부상 신고자수를 더한 값
info["부상자수"] = info["부상자수"].astype(int) # 정수형으로 형변환

info_column_add = info
print(info_column_add)





장르별 인기 있는 웹툰들 알아보기
어떤 사이트에서는 웹툰이 정기적으로 연재 중입니다. 이 사이트에 연재된 웹툰들의 데이터를 보고 장르별 인기 웹툰을 확인해보려고 합니다.

webtoon.csv 파일에 어떤 사이트에 연재된 웹툰들에 대한 데이터가 담겨 있습니다.

해당 CSV파일의 칼럼은 각각 다음의 의미가 있습니다.

칼럼	의미	예시
Name	웹툰 제목	Pyramid Game
Writer	웹툰 작가	Dalgonyak
Likes	‘좋아요’ 수	720233
Genre	장르	Drama
Rating	별점	9.53
Subscribers	구독자 수	173800
Likes와 Subscribers 칼럼은 정수, Rating 칼럼은 실수, 나머지 칼럼은 문자열 데이터를 담고 있습니다.
지시사항을 참고하여 코드를 작성하세요.


지시사항
함수 best()를 아래 내용을 참고하여 완성하세요.

매개변수는 다음과 같습니다.
매개변수	설명
df	웹툰에 대한 정보가 담긴 데이터 (데이터프레임)
g	알고 싶은 장르 (문자열)
주어진 df에서 장르가 g인 웹툰들에 대해 Rating 값이 가장 높은 데이터들을 데이터프레임 형태로 반환합니다.
해당 장르에서 Rating 값이 가장 높은 데이터는 한 개 이상입니다. (즉, Rating 값이 가장 높은 데이터가 여러 개인 경우에는 모두 출력해야 합니다)
단, 반환되는 데이터프레임은 기존의 모든 칼럼을 포함해야 합니다.
반환되는 형태는 데이터프레임이어야 합니다.


6. 생략


7. 장르별 인기 있는 웹툰들 알아보기
어떤 사이트에서는 웹툰이 정기적으로 연재 중입니다. 이 사이트에 연재된 웹툰들의 데이터를 보고 장르별 인기 웹툰을 확인해보려고 합니다.

webtoon.csv 파일에 어떤 사이트에 연재된 웹툰들에 대한 데이터가 담겨 있습니다.

해당 CSV파일의 칼럼은 각각 다음의 의미가 있습니다.

칼럼	의미	예시
Name	웹툰 제목	Pyramid Game
Writer	웹툰 작가	Dalgonyak
Likes	‘좋아요’ 수	720233
Genre	장르	Drama
Rating	별점	9.53
Subscribers	구독자 수	173800
Likes와 Subscribers 칼럼은 정수, Rating 칼럼은 실수, 나머지 칼럼은 문자열 데이터를 담고 있습니다.
지시사항을 참고하여 코드를 작성하세요.


지시사항
함수 best()를 아래 내용을 참고하여 완성하세요.

매개변수는 다음과 같습니다.
매개변수	설명
df	웹툰에 대한 정보가 담긴 데이터 (데이터프레임)
g	알고 싶은 장르 (문자열)
주어진 df에서 장르가 g인 웹툰들에 대해 Rating 값이 가장 높은 데이터들을 데이터프레임 형태로 반환합니다.
해당 장르에서 Rating 값이 가장 높은 데이터는 한 개 이상입니다. (즉, Rating 값이 가장 높은 데이터가 여러 개인 경우에는 모두 출력해야 합니다)
단, 반환되는 데이터프레임은 기존의 모든 칼럼을 포함해야 합니다.
반환되는 형태는 데이터프레임이어야 합니다.
함수 호출 예시

print(best(df, "Romance"))
Copy
출력 예시

아래의 예시는 문제를 푸는 데 참고할 수 있는 예시입니다. 최종적으로 제출해야 하는 답안과는 관계가 없습니다. 출력 예시를 그대로 반환하면 오답 처리됩니다.

                        Name                    Writer    Likes    Genre  Rating  Subscribers
324  See You in My 19th Life                   Lee Hey  5100000  Romance    9.89       945200
294       Seasons of Blossom         HONGDUCK / NEMONE  3200000  Romance    9.89       584900
323   My Gently Raised Beast  Yeoseulki / Early Flower  2300000  Romance    9.89       804700
Copy
Tips!
기본적으로 주어진 함수를 삭제하거나 지시사항에서 제시한 이름을 사용하지 않으면 채점 시 오류가 발생할 수 있으니 유의하시기 바랍니다.

# 아래 코드는 문제 해결을 위해 기본적으로 제공되는 코드입니다. 수정하지 마세요!
# 컬럼 순서 바꾸는 구문
# a = df.columns.tolist()
# df[["컬럼1","컬럼2","컬럼3" ]]
import pandas as pd

df = pd.read_csv("webtoon.csv")

# 지시사항을 참고하여 코드를 작성하세요.
def best(df, g):

    # df에서 장르가 g와 일치하는 행만을 필터링하고, 평점(Rating) 기준으로 내림차순 정렬하는 코드를 작성하세요
    df = df[df["Genre"]==g].sort_values("Rating",ascending=False)
    

    # 두줄로 만들수도 있음
    # df = df[df["Genre"]==g]
    # df =df.sort_values("Rating",ascending=False)
    #print(df)

    # 정렬된 데이터프레임의 첫 번째 행(가장 높은 평점)의 평점을 first 변수에 저장하는 코드를 작성하세요
    first = df["Rating"].iloc[0]  
    # 동일한 구문 first = df.iloc[0]['Rating'] 
    # print(first)
    
    # 평점이 가장 높은 웹툰들만 필터링하여 df를 업데이트하는 코드를 작성하세요
    df = df[df['Rating']==first]
    # 동일한 구문 df=df[df['Rating']==df['Rating'].max()]
    
    
    return df


# 결과를 확인하기 위한 코드입니다.
print(best(df, "Romance"))


8 생략

9. B라도 받자
엘리스 대학교의 데이터 분석 과목의 성적이 기록된 데이터를 보고 교수님이 정해두신 기준에 따라 B를 받을 수 있는 학생들을 구하려고 합니다.

scores.csv 파일에 엘리스 대학교의 데이터 분석 과목 성적 기록이 담겨 있습니다.

해당 CSV파일의 칼럼은 각각 다음의 의미가 있습니다.

칼럼	의미	형태	참고	예시
id	학번	정수	1~50	4
name	이름	문자열		Peter
age	나이	정수	19~29	29
midterm	중간고사 성적	정수	0~100	100
final	기말고사 성적	정수	0~100	47
total	환산 점수	실수	0.0~100.0	68.2
지시사항을 참고하여 코드를 작성하세요.


지시사항
함수 grade_B()를 아래 내용을 참고하여 완성하세요.

매개변수는 다음과 같습니다.

매개변수	설명
df	데이터프레임 형태의 성적 데이터
score	A를 받을 수 있는 최저 ‘환산 점수’
n	B를 받을 수 있는 학생 수
교수님은 A를 받지 못하는 학생 중 최종 성적에 대해 상위 n명까지 B를 주시려고 합니다.

주어진 매개변수들을 토대로 B를 받을 수 있는 학생들의 이름을 오름차순으로 정렬된 리스트로 반환합니다.

반환되는 형태는 리스트여야 합니다.

함수 호출 예시

print(grade_B(df, 75, 10))
Copy
출력 예시

['Frank', 'Jeremy', 'Katherine', 'Michael', 'Omar', 'Peter', 'Sandra', 'Teri', 'Tina', 'Victoria']
Copy
Tips!
기본적으로 주어진 함수를 삭제하거나 지시사항에서 제시한 이름을 사용하지 않으면 채점 시 오류가 발생할 수 있으니 유의하시기 바랍니다.



# 아래 코드는 문제 해결을 위해 기본적으로 제공되는 코드입니다. 수정하지 마세요!
import pandas as pd

df = pd.read_csv("scores.csv")

# 지시사항을 참고하여 코드를 작성하세요.
def grade_B(df, score, n):

    # 'total' 열의 값이 'score'보다 작은 행들만 필터링하여 새로운 DataFrame 'result'를 생성하는 코드를 작성하세요
    result = df[df['total']<score]

    #print(result) 
    
    # 'result' DataFrame을 'total' 열의 값에 따라 내림차순으로 정렬하는 코드를 작성하세요
    result = result.sort_values("total",ascending=False)

    # print(result)
    
    # 정렬된 'result' DataFrame에서 상위 'n'개의 행만을 선택하는 코드를 작성하세요
    result = result.head(n)
    # 동일 구문 result = result.iloc[0:n]
    print(result)
    
    
    # 'result' DataFrame을 'name' 열에 따라 오름차순으로 정렬하고, 'name' 열의 값들을 반환하는 코드를 작성하세요

    result=result.sort_values("name", ascending=True)
    result=list(result["name"])     # list 변환
    # 리스트 형태로 변환하여 반환해야합니다
    return result


# 결과를 확인하기 위한 코드입니다. 값을 변경해가며 테스트해 보세요!
print(grade_B(df, 75, 10))


10. 사고의 피해에 따른 교통사고 유형 찾기
도로교통공단에서 제공한 유형별 교통사고에 대한 데이터를 활용하여 유형별로 사고의 피해를 비교해보려고 합니다.

accident.csv 파일에 유형별 교통사고의 통계가 담겨있습니다.

해당 CSV파일의 칼럼은 각각 다음을 의미합니다.

칼럼	예시
사고유형대분류	차대사람
사고유형중분류	보도통행중
사고건수	2188
사망자수	45
중상자수	1005
경상자수	1239
부상신고자수	121
각 칼럼 내용의 의미는 칼럼의 이름이 잘 표현하고 있습니다.
부상신고자수의 경우, 교통사고로 인하여 5일 미만의 치료를 요하는 부상을 입은 사람의 수를 뜻합니다.
지시사항을 참고하여 코드를 작성하세요.


지시사항
함수 danger()를 아래 내용을 참고하여 완성하세요.

데이터프레임 형태의 교통사고 통계 데이터 df와 알고 싶은 사고 당 피해의 순위 n을 매개변수로 받습니다.
주어진 df에 사고 당 피해라는 칼럼을 추가합니다. 이때 사고 당 피해의 값은 전체 사상자수를 사고건수로 나눈 값입니다. 사상자수는 사망자수, 중상자수, 경상자수, 부상신고자수를 모두 포함합니다.
사고 당 피해의 순위가 n인 사고의 사망자수와 중상자수의 합을 구하여 반환합니다.
반환되는 형태는 정수형이어야 합니다.
함수 호출 예시

print(danger(df, 1))
Copy
출력 예시

아래의 예시는 문제를 푸는 데 참고할 수 있는 예시입니다. 최종적으로 제출해야 하는 답안과는 관계가 없습니다. 출력 예시를 그대로 반환하면 오답 처리됩니다.

4250
Copy
결과는 주어진 예시와 같은 형식으로 출력되어야 합니다.

# 아래 코드는 문제 해결을 위해 기본적으로 제공되는 코드입니다. 수정하지 마세요!
import pandas as pd

df = pd.read_csv("accident.csv")

# 지시사항을 참고하여 코드를 작성하세요.
def danger(df, n):
    
    # DataFrame에 '전체'라는 새로운 열을 추가하는 코드를 작성하세요
    # 이 열은 사망자수, 중상자수, 경상자수, 부상신고자수의 합을 나타냅니다.

    df["전체"] = df["사망자수"]+df["중상자수"]+df["경상자수"]+df["부상신고자수"]
    
    
    # DataFrame에 '사고당피해'라는 새로운 열을 추가하는 코드를 작성하세요
    # 이 열은 전체 피해자 수를 사고건수로 나눈 값을 나타냅니다
    df["사고당피해"] = df["전체"]/ df["사고건수"]

    
    
    # DataFrame을 '사고당피해' 열의 값에 따라 내림차순으로 정렬하는 코드를 작성하세요
    df = df.sort_values("사고당피해", ascending=False)

    
    
    # 정렬된 DataFrame에서 n번째 행을 선택하는 코드를 작성하세요
    #1번째 행은 0부터 시작하므로 n-1 로 해야한다
    result = df.iloc[n-1]
    
    
    
    # 선택된 행의 '사망자수'와 '중상자수'를 합산하여 반환하는 코드를 작성하세요
    return int(result['사망자수']+result['중상자수']) # 정수로 반환해서 int() 묶어주기


# 값을 확인하기 위한 코드입니다. 값을 변경해가며 테스트해 보세요!
print(danger(df, 1))




##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################

##########################################################################################################################################################

##########################################################################################################################################################

##########################################################################################################################################################

##########################################################################################################################################################

#######################################################################################################################################################################################################################################

##########################################################################################################################################################





# JSON 데이터 다루기

**[목차]**
* JSON이란?
* JSON 데이터 다루기


## 1. JSON 이란?
JSON(JavaScript Object Notation)은 **키-값 쌍으로 이루어진 데이터**를 전달 또는 저장하기 위해 사람이 사용할 수 있는 텍스트를 사용한 개방형 표준 포맷 </br>
주로 웹 애플리케이션과 다양한 소프트웨어 간에 데이터를 전송하거나 저장하는 용도로 사용 </br></br>

JSON은 단순한 데이터 포맷일 뿐이며 어떠한 통신 방법도 아니고 프로그래밍 문법도 아님


### 1.1 JSON 데이터 구성
JSON의 각 키(KEY)는 문자열로 표현되며 키와 값(VALUE)은 콜론(:)으로 구분 </br>
JSON은 `문자열`, `숫자`, `불리언`, `객체`, `배열`, `null` 다양한 데이터 타입을 지원
키 : 값 , 키: 값, ,,,


### 1.2 JSON 데이터 예시
```json
{
    "employee": {
        "id": 123, 
        "name": "Queen", 
        "department": "HR", 
        "skills": ["communication", "teamwork"]
    }
}
```

## 2. JSON 데이터 다루기
데이터를 분석하고 원하는 데이터를 추출하는 과정을 **데이터 파싱(Data Parsing)** 이라 함 </br>
예를 들면, 주행 중에 발생하고 json 타입으로 저장된 데이터에서 원하는 데이터를 추출하기 위해서는 데이터 타입에 맞는 파싱 방법이 필요

### 2.1 JSON Library

* Python 에는 `json 라이브러리`가 내장되어 있어 JSON 데이터를 처리할 수 있음 </br>
* 내장 라이브러리이기 때문에 별도의 설치 과정이 필요 없음 </br>
* JSON 형식으로 된 문자열 또는 파일을 읽고 Python 객체(Dictionary or List type)로 만들어서 분석함

# json 라이브러리 호출하는 방법
import json

### 2.2 json.load

JSON 파일 객체를 Python에서 사용할 수 있는 객체로 변환하는 함수 

# targer.json 파일을 읽어 Python 객체로 변환
with open('data/target.json') as json_file :   #('data/target.json', r) r이 생략 read
    #parsed_data = None
    parsed_data = json.load(json_file)

print(parsed_data)

파싱된 데이터가 딕셔너리 형태일 경우, 기존의 딕셔너리 형태를 사용하는 것과 동일하게 사용할 수 있음

print(type(parsed_data)) #딕셔너리 형태로 저장

print(parsed_data["employee"])

print(parsed_data["employee"]['skills'])

print(type(parsed_data["employee"]['skills']))

### 2.3 json.dump
Python 객체를 JSON 파일 객체로 변환하는 함수

##### Dictionary → JSON

json_data_dic = {"name" : "elice", "age" : 25, "email" : "rabbit@elicer.com"}
print(type(json_data_dic))

with open("create_file1.json", "w") as f :   # w 쓰기 write
    #None
    json.dump(json_data_dic, f)  # "create_file1.json" =f로 명명

##### List → JSON

json_data_list = [{"name" : "elice", "age" : 25}, {"name" : "Queen", "age" : 47}, {"name" : "Carrot", "age" : 1}]
print(type(json_data_list))

with open("create_file2.json", "w") as f :
    #None
    json.dump(json_data_list,f)

### [TODO] JSON 파일을 파싱하여 새로 저장하기
* data 폴더에 있는 `parsing_target.json` 은 자동차 주행 중에 센서로 습득한 데이터의 일부분입니다.
* 해당 데이터를 읽고 GPS 위도(latitude)와 경도(longitude) 데이터를 추출하여 `result1.json`에 저장하는 코드를 작성하세요.

json_path = "data/parsing_target.json"# json_path 변수에 경로를 저장해뒀다

# JSON 파일을 읽는 코드를 작성해주세요.
#None
with open(json_path, "r") as f:
    parsed_data=json.load(f)

print(parsed_data)

# 읽은 파일에서 gps 위도(latitude)와 경도(longitude) 데이터를 파싱하여 저장하는 코드를 작성해주세요.
#latitude = None
#longitude = None
latitude =parsed_data['gps_latitude']
longitude =parsed_data['gps_longitude']

# 위도와 경도 데이터를 result1.json 파일로 저장하는 코드를 작성해주세요.
# 파일 경로가 달라지면 채점이 정상적으로 동작하지 않을 수 있습니다.
gps_data = [latitude, longitude]

with open("result1.json", "w") as f :
    json.dump(gps_data, f)

### 2.4 json.loads
문자열로 표현된 JSON 데이터를 Python 객체로 변환하는 함수

str_json = '{"employee": {"id": 123, "name": "Queen", "department": "HR", "skills": ["communication", "teamwork"]}}'

print(type(str_json))
# 기존 json 데이터를 '' 따옴표로 묶어서 문자열로 취급되게 한다
# '{"employee": {"id": 123, "name": "Queen", "department": "HR", "skills": ["communication", "teamwork"]}}'
# -> ' '

loads_data = json.loads(str_json)
print(loads_data)

print(type(loads_data))

print(loads_data["employee"]['skills'])
print(type(loads_data["employee"]['skills']))

### 2.5 json.dumps
Python 객체를 문자열로 표현된 JSON 데이터로 변환하는 함수

dic_data = {"employee": {"id": 123, "name": "Queen", "department": "HR", "skills": ["communication", "teamwork"]}}
print(type(dic_data))

dumps_data = json.dumps(dic_data)
print(dumps_data)
print(type(dumps_data))

* dumps_data를 출력하면 한 줄로 출력되어 가독성이 매우 떨어짐
* `dumps` 함수에 *indent* 인자를 추가하면 들여쓰기를 할 수 있음

dumps_data2 = json.dumps(dic_data, indent=4) # 가독성 좋게 4만큼 띄워쓰기 해서 들여쓰기 된다
print(dumps_data2)


##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################

##########################################################################################################################################################

##########################################################################################################################################################

##########################################################################################################################################################

##########################################################################################################################################################

#######################################################################################################################################################################################################################################

##########################################################################################################################################################





# JSON 데이터와 데이터프레임


**[목차]**
- JSON to DataFrame
- DataFrame to JSON

## 1. JSON to DataFrame
`Pandas 라이브러리`를 사용하면 JSON 파일을 데이터프레임(DataFrame)으로 변환할 수 있음 </br>
변환된 데이터프레임을 이용하면 다양한 데이터 분석 및 시각화 라이브러리가 이용 가능하여 효과적인 데이터 분석 가능

import json
import pandas as pd

### 1.1 json.load로 불러오기

# targer.json 파일을 읽어 Python 객체로 변환
with open('data/target.json') as json_file :
    load_data = json.load(json_file)

# load한 json 데이터 확인
print(load_data)
print(type(load_data))

# load한 json 데이터 dumps로 확인
dumps_data=json.dumps(load_data, indent=4)
print(dumps_data)

load_df1 =pd.DataFrame(load_data) 
#판다스 데이터 프레임 만들떄 
# df = pd.
load_df1

### 1.2 pd.read_json 으로 읽기

# 1.load.json()을 통해 딕셔너리(파이썬 객채로 읽기)
# 2. 위에 1.1에서 한거 처럼 DataFrame으로 만들기를 아래로 바로 만들수 있다

load_df2 = pd.read_json('data/target.json')


load_df2

### 1.3 불러온 데이터프레임으로 데이터 분석하기
employee에서 **skill의 빈도수를 도출**하는 간단한 데이터 분석을 진행

# Python 객체를 DataFrame으로 변환
df = pd.DataFrame(load_data["employee"])
df

# DataFrame의 'skills' 열을 이용하여 각 기술을 별도의 행으로 분리
# 이를 위해 'explode' 메서드를 사용
# explode : 리스트 언패킹
# 리스트와 같은 요소를 행으로 변환하여 인덱스 값을 복제하게 됩니다
# 복제된 값으 인덱스는 이전 인덱스의 값과 동일한 값으로 저장됩니다
df_skills = df['skills'].explode()
df_skills

# 각 기술의 빈도수 계산
# value_counts() : 각 value의 개수를 세는 함수
# reset index : 인덱스를 새로 부여. Value+counts() 함수로 인해 인덱스가 skil명으로 바뀌었습니다
# reset_index() 를 사용하면, 인덱스를 0부터 새로 부여해서, 재정렬이 되는 효과를 볼 수 있다

skills_frequency_df = df_skills.value_counts().reset_index()

skills_frequency_df
# reset사용시 콜롬 명이 이상하게 변해서 아래와 같이 이름 명확하게 변경 필요

# 열 이름을 명확하게 변경
skills_frequency_df.columns = ['Skill', 'Frequency']    
skills_frequency_df

# 메트플롯 라이브러리 : 그래프를 그려주는 라이브러리
import matplotlib.pyplot as plt

# 데이터프레임으로부터 막대 그래프를 생성
# 그릴 그래프의 사이즈를 지정
plt.figure(figsize=(10, 6))

# plt.bar(x축 데이터, y축 데이터, color= 색상 정보)
plt.bar(skills_frequency_df['Skill'], 
        skills_frequency_df['Frequency'], 
        color='skyblue'
        )

# 그래프 꾸며주는 부가적인 기능 
# 그래프 제목과 라벨을 추가, 
plt.title('Frequency of Skills')

# X축 라벨 (정보)
plt.xlabel('Skill')
# Y축 라벨 (정보)
plt.ylabel('Frequency')

# x축에 있는 기술 이름을 회전시켜 가독성 상승
plt.xticks(rotation=45)

# 그래프를 표시
# 회전된 x축 라벨을 위해 레이아웃을 조정
plt.tight_layout()  
plt.show()


## 2. DataFrame to JSON
Pandas 라이브러리에서 지원하는 `to_json 함수`를 사용하면 분석한 데이터프레임을 JSON으로 저장할 수 있음 </br>
단, Series 자료형은 JSON으로 저장할 수 없음

### 2.1 새로운 데이터 추가하기

#새로운 데이터 프레임 생성

new_employee = [{
        "id" : 108, 
        "name" : "King", 
        "department" : "Developer", 
        "skills" : ["leadership", "communication"]
    },
    {
        "id" : 133, 
        "name" : "KIM", 
        "department" : "Marketer", 
        "skills" : ["teamwork", "decisiveness"]
    }]


new_df = pd.DataFrame(new_employee)
new_df

df_concat = pd.concat([df, new_df], ignore_index=True)
df_concat

# to_json 함수를 이용해서 저장
# 인자로 orient='records' 로 저장
# 저장할 파일명 : employee_data.json
df_concat.to_json("employee_data.json",orient="records") 
# orient="records" 리스트의 형태로 묶이게 되며 각각의 요소가 분리돼서 저장
# 보기 편한방법이 records 방법을 많이 쓴다

# 저장 방식 orient 라는 속성을 지정할 수 있다
# df_concat.to_json("employee_data.json",orient="records") 
# df_concat.to_json("employee_data.json",orient="columns") 
# df_concat.to_json("employee_data.json",orient="split") 
# df_concat.to_json("employee_data.json",orient="values") 

# employee_data.json 파일을 읽어서 확인하기
with open('employee_data.json') as employee_data :
    load_data = json.load(employee_data)

print(json.dumps(load_data, indent=4))

